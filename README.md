# Configuration-Management-and-the-Cloud
Writing of lectures

## Automating with Configuration Management

### What is scale?

In this course we'll focus on making our work scale. So what do we mean when we talk about scale? **Being able to scale what we do means that we can keep achieving larger impacts with the same amount of effort when a system scales**. Well an increase in the amount of work it needs to do can be accommodated by an increase in capacity. For example, if the web application your company provides is scalable, that it can handle an increase in the number of people using it by adding more servers to serve requests. In short, a scalable system is a flexible one. Adding more computers to the pool of servers that are serving the website can be a very simple or very hard operation depending on how your infrastructure is set up. To figure out how scalable your current setup is, you can ask yourself questions like will adding more servers increase the capacity of the service? How are new servers prepared, installed, and configured? How quickly can you set up new computers to get them ready to be used? Could you deploy a hundred servers with the same IT team that you have today? Or would you need to hire more people to get it done faster? Would all the deployed servers be configured exactly the same way? Scaling isn't just about website serving content of course. If your company is rapidly hiring a lot of new employees, you'll need to have an onboarding process that can scale as needed. And as you keep adding new computers to the network, you'll need to make sure that your system administration process can scale to the growing needs of the company. This can include tasks like a applying the latest security policies and patches while making sure users' needs still get addressed all while more and more users join the network without new support staff to back you up. If making this happen sounds a bit like magic right now, remember that we're here to share the secret ingredient with you, automation. **Automation is an essential tool for keeping up with the infrastructure needs of a growing business.** By using the right automation tools, we can get a lot more done in the same amount of time. For example, we could deploy a whole new server by running a single command and letting the automation take care of the rest. We could also create a batch of user accounts with all the necessary permissions based on data already stored in the database, eliminating all human interaction. Automation is what lets us scale. It allows a small IT team to be in charge of hundreds or even thousands of computers. Okay, so what does that look like in practice? There's a bunch of different tools that we can use to achieve this. Up next, we'll talk about a type of tool called configuration management that can help us automate how we manage the computers in our fleets.

### What is configuration management?

Imagine your team is in charge of setting up a new server. This could be a physical computer running close to you or a virtual machine running somewhere in the cloud. To get things moving, the team installs the operating system, configures some applications and services, sets up the networking stack, and when everything is ready, puts the server into use. By manually deploying the installation and configuring the computer, we see that we're using unmanaged configuration. When we say configuration here, we're talking about everything from the current operating system and the applications installed to any necessary configuration files or policies, including anything else that's relevant for the server to do its job. When you work in IT, you're generally in charge of the configuration of a lot of different devices, not just servers. Network routers printers and even smart home devices can have configuration that we can control. For example, a network switch might use a config file to set up each of its ports. All right, so now we know what we mean when we talk about configuration. We said that manually deploying a server means that the configuration is unmanaged. **So what would it mean for the configuration to be managed? It means using a configuration management system to handle all of the configuration of the devices in your fleet, also known as nodes.** There's a bunch of different tools available depending on the devices and services involved. Typically you'll define a set of rules that have to be applied to the nodes you want to manage and then have a process that ensures that those settings are true on each of the nodes. At a small scale, unmanaged configurations seem inexpensive. If you only manage a handful of servers, you might be able to get away with doing that without the help of automation. You could log into each device and make changes by hand when necessary. And when your company needs a new database server, you might just go ahead and manually install the OS and the database software into a spare computer. But this approach doesn't always scale well. The more servers that you need to deploy, the more time it will take you to do it manually. And when things go wrong, and they often do, it can take a lot of time to recover and have the servers back online. Configuration management systems aim to solve this scaling problem. By managing the configuration of a fleet with a system like this, large deployments become easier to work with because the system will deploy the configuration automatically no matter how many devices you're managing. When you use configuration management and you need to make a change in one or more computers, you don't manually connect to each computer to perform operations on it. Instead, you edit the configuration management rules and then let the automation apply those rules in the affected machines. This way the changes you make to a system or group of systems are done in a systematic, repeatable way. Being repeatable is important because it means that the results will be the same on all the devices. A configuration management tool can take the rules you define and apply them to the systems that it manages, making changes efficient and consistent. Configuration management systems often also have some form of automatic error correction built in so that they can recover from certain types of errors all by themselves. For example, say you found that some application that was being used widely in your company was configured to be very insecure. You can add rules to your configuration management system to improve the settings on all computers. And this won't just apply the more secure settings once. It will continue to monitor the configuration going forward. If a user changes the settings on their machine, the configuration management tooling will detect this change and reapply the settings you defined in code. How cool is that? **There are lots of configuration management systems available in the IT industry today. Some popular systems include Puppet, Chef, Ansible, and CFEngine.** These tools can be used to manage locally hosted infrastructure. Think bare metal or virtual machines, like the laptops or work stations that employees use at a company. Many also have some kind of Cloud integration allowing them to manage resources in Cloud environments like Amazon EC2, Microsoft Azure, or the Google Cloud platform, and the list doesn't stop there. There are some platform specific tools, like SCCM and Group Policy for Windows. These tools can be very useful in some specific environments, even when they aren't as flexible as the others. For this course, we've chosen to focus on Puppet because it's the current industry standard for configuration management. Keep in mind though that selecting a configuration management system is a lot like deciding on a programming language or version control system. You should pick the one that best fits your needs and adapt accordingly, if necessary. Each has its own strengths and weaknesses. So a little research beforehand can help you decide which system is best suited for your particular infrastructure needs. There are a lot of tools out there. So be sure to check them out. Up next, we'll discuss how we can make the most out of our configuration management system using the infrastructure as code paradigm.

### What is infrastructure as code?

We've called out that when we use a configuration management system, we write rules that describe how the computers in our fleet should be configured. These rules are then executed by the automation, to make the computers match our desired state. This means that we can model the behavior of our IT infrastructure in files that can be processed by automatic tools. These files can then be tracked in a version control system. Remember, version control systems help us keep track of all changes done to the files, helping answer questions like who, when, and why. More importantly, they're super-useful when we need to revert changes. This can be especially helpful if a change turns out to be problematic. **The paradigm of storing all the configuration for the managed devices in version controlled files is known as Infrastructure as Code or IaC.** In other words, we see that we're using Infrastructure as Code when all of the configuration necessary to deploy and manage a node in the infrastructure is stored in version control. This is then combined with automatic tooling to actually get the nodes provisioned and managed. If you have all the details of your Infrastructure properly stored in the system, you can very quickly deploy a new device if something breaks down. Simply get a new machine, either virtual or physical, use the automation to deploy the necessary configuration, and you're done. **The principals of Infrastructure as Code are commonly applied in cloud computing environments, where machines are treated like interchangeable resources, instead of individual computers. This principle is also known as treating your computers as cattle instead of pets because you care for them as a group rather than individually.** Apologies to anyone with a pet cow. This concept isn't just for managing computers in huge data centers or globe spanning infrastructures, it can work for anything; from servers to laptops, or even workstations in a small IT department. Even if your company only has a single computer working as the mail server, you can still benefit from storing all the configuration needed to set it up in a configuration management system. That way if the server ever stops working, you can deploy a replacement very quickly by simply applying the rules that configure the mail server to the new computer. One valuable benefit of this process is that the configuration applied to the device doesn't depend on a human remembering to follow all the necessary steps. Rest assured, silly human, the result will always be the same, making the deployment consistent. As mentioned, having Infrastructure as Code means that we can also apply the benefits of the version control system or VCS to your infrastructure. Since the configuration of our computers is stored in files, those files can be added to a VCS. This has all the benefits that version control systems bring. It gives us an audit trail of changes, it lets us quickly rollback if a change was wrong, it lets others reviewed our code to catch errors and distribute knowledge, it improves collaboration with the rest of the team, and it lets us easily check out the state of our infrastructure by looking at the rules that are committed. Not too shabby. I personally think this is one of the coolest things about IaC. The ability to easily see what configuration changes were made and roll back to a known good state is super important. It can make a big difference in quickly recovering from an outage, especially since changing the contents of the configuration file can be as dangerous as updating the version of an application. I've had my fair share of outages caused by an innocent-looking change with unintended side effects. But storing all the infrastructure in a version control system lets me quickly roll back to a previously known good version so that the outage length can be minimized. On top of that, having the rules stored in files means that we can also run automated tests on them. It's much better to find out in a test that a configuration file has a typo in it than to find out from our users. In a complex or large environment, treating your IT Infrastructure as Code can help you deploy a flexible scalable system. A configuration management system can help you manage that code by providing a platform to maintain and provision that infrastructure in an automated way. Having your infrastructure stored as code means that you can automatically deploy your infrastructure with very little overhead. If you need to move it to a different location, it can be deployed, de-provisioned, and redeployed at scale in a different locale with minimal code level changes. **To sum all of this up, managing your Infrastructure as Code it means that your fleet of nodes are consistent, versioned, reliable, and repeatable.** Instead of being seen as precious or unique, machines are treated as replaceable resources that can be deployed on-demand through the automation. Any infrastructure that claims to be scalable must be able to handle the capacity requirements of growth. Performing an action like adding more servers to handle an increase in requests is just a possible first step. There are other things that we might need to take into account, such as the amount of traffic that network can handle or the load on the back end servers like databases. Viewing your infrastructure in this way helps your IT team adapt and stay flexible. The technology industry is constantly changing and evolving. Automation and configuration management can help you embrace that change instead of avoiding it. Before diving into concrete examples of what this looks like, the first practice quiz of the course is coming up. These quizzes act as check-in points to help you make sure all the concepts covered in the videos are making sense. See you on the other side.

### What is Puppet?

As we called out a couple of times already, in this course, we'll be learning how to apply basic configuration management concepts by using Puppet. **Puppet is the current industry standard for managing the configuration of computers in a fleet of machines.** Part of the reason why Puppet is so popular is that it's a cross-platform tool that's been around for a while. It's an open source project that was created in 2005, and it's gone through several different versions. As it's evolved, the tool has incorporated feedback from its users to make it more and more useful. The latest available version at the time this Google course went live is Puppet 6, which came out in late 2018. **We typically deploy puppet using a client-server architecture. The client is known as the Puppet agent, and the service is known as the Puppet master.** When using this model, the agent connects to the master and sends a bunch of facts that describe the computer to the master. The master then processes this information, generates the list of rules that need to be applied on the device, and sends this list back to the agent. The agent is then in charge of making any necessary changes on the computer. **Puppet is a cross-platform application available for all Linux distributions, Windows, and Mac OS. This means that you can use the same puppet rules for managing a range of different computers.** What are these rules that we keep talking about? Let's check out a very simple example. This block is saying that the package 'sudo' should be present on every computer where the rule gets applied. If this rule is applied on 100 computers, it would automatically install the package in all of them. This is a small and simple block but can already give us a basic impression of how rules are written in puppet. Don't worry too much about the syntax now, we'll look into what each piece means in future videos. There are various installation tools available depending on the type of operating system. Puppet will determine the type of operating system being used and select the right tool to perform the package installation. **On Linux distributions, there are several package management systems like APT, Yum, and DNF.** Puppet will also determine which package manager should be used to install the package. On Mac OS, there's a few different available providers depending on where the package is coming from. **The Apple Provider is used for packages that are part of the OS, while the MacPorts provider is used for packages that come from the MacPorts Project. For Windows, we'll need to add an extra attribute to our rule, stating where the installer file is located on the local desk or a network mounted resource.** Puppet will then execute the installer and make sure that it finishes successfully. If you use Chocolatey to manage your windows packages, you can add an extra Chocolatey provider to Puppet to support that. We'll add a link to more information about this in our next reading. **Using rules like this one, we can get puppet to do a lot more than just install packages for us. We can add, remove, or modify configuration files stored in the system, or change registry entries on Windows. We can also enable, disable, start, or stop the services that run on our computer.** We can configure crone jobs, the scheduled tasks, add, remove, or modify Users and Groups or even execute external commands, if that's what we need. There's a lot to say about puppet. We won't go into absolutely every detail, but we'll cover the most important concepts in this course. The goal is to get you started with what you need to know about configuration management in general and puppet in particular. We'll also give you pointers to find out more information on your own. Up next, we'll check out the different resources we can use to define our rules.

### Puppet Resources

In our last video, we saw an example that installed the pseudo package in a computer. To do that, our example used the package keyword declaring a package resource. **In puppet, resources are the basic unit for modeling the configuration that we want to manage. In other words, each resource specifies one configuration that we're trying to manage, like a service, a package, or a file.** Let's look at another example. In this case, we're defining a file resource. This resource type is used for managing files and directories. In this case, it's a very simple rule that ensures that etc/sysctl.d exists and is a directory. Let's talk a little bit about syntax. In both our last example and this one we could see that when declaring a resource in puppet, we write them in a block that starts with the resource type ,in this case File. The configuration of the resource is then written inside a block of curly braces. Right after the opening curly brace, we have the title of the resource, followed by a colon. After the colon come the attributes that we want to set for the resource. In this example, we're once again setting the insurer attribute with directory as the value, but we could set other attributes too >> Let's check out a different file resource. In this example, we're using a file resource to configure the contents of etc/timezone, a file, that's used in some Linux distributions to determine the time zone of the computer. This resource has three attributes. First, we explicitly say that this will be a file instead of a directory or a symlink then we set the contents of the file to the UTC time zone. Finally, we set the replace attribute to true, which means that the contents of the file will be replaced even if the file already exists. We've now seen a couple examples of what we can do with the file resource. There are a lot more attributes that we could set, like file permissions the file owner, or the file modification time.
We've included a link to the official documentation in the next reading where you can find all the possible attributes that can be set for each resource. How do these rules turn into changes in our computers? When we declare a resource in our puppet rules. We're defining the desired state of that resource in the system. The puppet agent then turns the desired state into reality using providers.
**The provider used will depend on the resource defined and the environment where the agent is running.** Puppet will normally detect this automatically without us having to do anything special. When the puppet agent processes a resource, it first decides which provider it needs to use, then passes along the attributes that we configured in the resource to that provider. **The code of each provider is in charge of making our computer reflect the state requested in the resource.** In these examples, We've looked at one resource at a time. Up next, we'll see how we can combine a bunch of resources into more complex puppet classes.

### Puppet Classes

In the examples of Puppet code that we've seen so far, we've declared classes that contain one resource. You might have wondered what those classes were for. We use these classes to collect the resources that are needed to achieve a goal in a single place. For example, you could have a class that installs a package, sets the contents of a configuration file, and starts the service provided by that package. Let's check out an example like that. In this case, we have a class with three resources, a package, a file, and a service. All of them are related to the Network Time Protocol, or NTP, the mechanism our computers use to synchronize the clocks. Our rules are making sure that the NTP package is always upgraded to the latest version. We're setting the contents of the configuration file using the source attribute, which means that the agent will read the required contents from the specified location. And we're saying that we want the NTP service to be enabled and running. By grouping all of the resources related to NTP in the same class, we only need a quick glance to understand how the service is configured and how it's supposed to work. This would make it easier to make changes in the future since we have all the related resources together. It makes sense to use this technique whenever we want to group related resources. For example, you could have a class grouping all resources related to managing log files, or configuring the time zone, or handling temporary files and directories. You could also have classes that group all the settings related to your web serving software, your email infrastructure, or even your company's firewall. We're just getting started with Puppet's basic resources and seeing how they can be applied. In further videos, we'll be learning a lot more about common practices when using configuration management tools. But before jumping into that, we've put together a reading with more information about Puppet syntax, resources and links to the official reference. Then we've got a quick quiz to check that everything is making sense.
**By grouping related resources together, we can more easily understand the configuration and make changes in the future.**

### Links 

Check out the following links for more information:

https://puppet.com/docs/puppet/latest/lang_resources.html
https://puppet.com/blog/deploy-packages-across-your-windows-estate-with-bolt-and-chocolatey/

### What are domain-specific languages?

Up until now, we've seen examples of very simple Puppet rules they just define whiner more resources. **These resources are the building blocks of Puppet rules, but we can do much more complex operations using Puppet's domain specific language or DSL.** **Typical programming languages like Python, Ruby, Java or Go are general purpose languages that can be used to write lots of different applications with different goals and use cases.** On the flip side, **a domain specific language is a programming language that's more limited in scope.** Learning a domain-specific language is usually much faster and easier than learning a general purpose programming language because there's a lot less to cover. You don't need to learn as much syntax or understand as many keywords or taking to account a lot of overhead in general. In the case of Puppet, the DSL is limited to operations related to when and how to apply configuration management rules to our devices. For example, we can use the mechanisms provided by the DSL to set different values on laptops or desktop computers, or to install some specific packages only on the company's web servers. On top of the basic resource types that we already checked out, Puppet's DSL includes variables, conditional statements, and functions. Using them, we can apply different resources or set attributes to different values depending on some conditions. Before we jump into an example of what that looks like, let's talk a bit about **Puppet facts. Facts are variables that represent the characteristics of the system.** __When the Puppet agent runs, it calls a program called factor which analyzes the current system, storing the information it gathers in these facts.__ Once it's done, it sends the values for these facts to the server, which uses them to calculate the rules that should be applied. Puppet comes with a bunch of baked-in core facts that store useful information about the system like what the current OS is, how much memory the computer has whether it's a virtual machine or not or what the current IP address is. If the information we need to make a decision isn't available through one of these facts, we can also write a script that checks for the information and turns it into our own custom fact. Let's check out an example of a piece of Puppet code that makes use of one of the built-in facts. **This piece of code is using the is-virtual fact together with a conditional statement to decide whether the smartmontools package should be installed or purged. This package is used for monitoring the state of hard drives using smart.** So it's useful to have it installed in physical machines, but it doesn't make much sense to install it in our virtual machines. We can see several of the characteristics of Puppets domain specific language in this block. So let's spend a little time looking at all of the elements of syntax here. First, facts is a variable. All variable names are preceded by a dollar sign in Puppet's DSL. **In particular, the facts variable is what's known as a hash in the Puppet DSL, which is equivalent to a dictionary in Python.** This means that we can access the different elements in the hash using their keys. In this case, we're accessing the value associated to the is virtual key. Second, we see how we can write a conditional statement using if else, enclosing each block of the conditional with curly braces. Finally, each conditional block contains a package resource. We've seen resources before, but we haven't looked at the syntax in detail. So let's do that now. Every resource starts with the type of resource being defined. In this case, package and the contents of the resource are then enclosed in curly braces. Inside the resource definition, the first line contains the title followed by a colon. Any lines after that are attributes that are being set. We use equals greater than to assign values to the attributes and then each attribute ends with a comma. We've now covered a large chunk of puppet's DSL syntax. If you look back to what it was like to learn your first programming language, you'll probably notice how much less syntax there is to learn here. That's typical of the domain specific languages used by configuration management tools. While each tool uses their own DSL, they're usually very simple and can be learned very quickly. Up next, we'll talk about a few other principles behind most configuration management tools. Whenever you're ready, let's dive in.
**A fact is a hash that stores information about the details of a particular system.**

### The Driving Principles of Configuration Management

Up to now, we've seen a few examples of what Puppet rules look like, including a bunch of different resources and even a conditional expression. You might have noticed that in all the examples we've checked out, we were never telling the computer the steps it should follow in order to do what we wanted. Instead, we were just declaring the end goal that we wanted to achieve, like going to a drive-through and ordering a burger, we didn't make it, but there it is. **The providers that we mentioned earlier lake apt and yum are the ones in charge of turning our goals into whatever actions are necessary.** We say that Puppet uses a **declarative language** because we declare the state that we want to achieve rather than the steps to get there. **Traditional languages like Python or C are called procedural because we write out the procedure that the computer needs to follow to reach our desired goal.** Coming from a procedural language like Python, it might take some time to get used to writing declarative code like the ones used for Puppet, and that's okay. Just remember that when it comes to configuration management, it makes sense to simply state what the configuration should be, not what the computer should do to get there. Say you're using a resource to declare that you want a package installed, you don't care what commands a computer has to run you install it, you only care that after the configuration management tool has run, the package is installed. **Another important aspect of configuration management is that operations should be idempotent. In this context, an idempotent action can be performed over and over again without changing the system after the first time the action was performed, and with no unintended side effects.** Let's check this out with an example of a file resource. This resource ensures that the /etc/issue file has a set of permissions and a specific line in it. Fulfilling this requirement is an idempotent operation. If the file already exists and has the desired content, then Puppet will understand that no action has to be taken. If the file doesn't exist, then puppet will create it. If the contents or permissions don't match, Puppet will fix them. No matter how many times the agent applies the rule, the end result is that this file will have the requested contents and permissions. **Idempotency is a valuable property of any piece of automation. If a script is idempotent, it means that it can fail halfway through its task and be run again without problematic consequences.** Say you're running your configuration management system to setup a new server. Unfortunately, the setup fails because you forgot to add a second disk to the computer and the configuration required two disks. If your automation is idempotent, you can add the missing disk and then have the system pick up from where it left off. Most Puppet resources provide idempotent actions, and we can rest assured that two runs of the same set of rules will lead to the same end result. **An exception to this is the exec resource, which runs commands for us.** The actions taken by the exec resource might not be idempotent since a command might modify the system each time it's executed. To understand this, let's check out what happens when we execute a command that moves a file on our computer. First, we'll check that the example.txt file is here, and then we'll move it to the desktop directory.
This works fine now, but what happens if we run the exact same command again after it's been executed once? **We receive an error because the file is no longer in the same place. In other words, this was not an idempotent action, as executing the same action twice produced a different result and the unintended side effect of an error.** If we were running this inside Puppet, this would cause our Puppet run to finish with an error. So if we need to use the exec resource to run a command for us, we need to be careful to ensure that the action is idempotent. **We could do that for example by using the onlyif attribute like this. Using the onlyif attribute, we specified that this command should be executed only if the file that we want to move exists. This means that the file will be moved if it exists and nothing will happen if it doesn't.** By adding this conditional, we've taken an action that's not idempotent and turned it into an idempotent one. Another important aspect of how configuration management works is the **test and repair paradigm. This means that actions are taken only when they are necessary to achieve a goal.** Puppet will first test to see if the resource being managed like a file or a package, actually needs to be modified. If the file exists in the place we want it to, no action needs to be taken. If a package is already installed, there's no need to install it again. This avoids wasting time doing actions that aren't needed. Finally, another **important characteristic is that Puppet is stateless, this means that there's no state being kept between runs of the agent.** Each Puppet run is independent of the previous one, and the next one. **Each time the puppet agent runs, it collects the current facts. The Puppet master generates the rules based just on those facts, and then the agent applies them as necessary.** We're just getting started with what configuration management is and what it looks like in Puppet. But hopefully, you're starting to see how understanding these basic concepts and how turning them into practical rules can help you manage a small army of computers. Up next, there's a reading with links to more information about the concepts we've covered followed by a quick quiz. You've got this.

### Links 

Check out the following links for more information:

https://en.wikipedia.org/wiki/Domain-specific_language
http://radar.oreilly.com/2015/04/the-puppet-design-philosophy.html


### Module 1 Wrap Up: Automating with Configuration Management

We started our journey talking about what would happen if you needed to upgrade a package in a fleet of 1,000 different servers. If you've never heard about configuration management before, an upgrade like that probably seemed like a super long and boring task, right? But now you know that there's a bunch of tools you can use to make your life much easier when making large-scale changes like that one. We've talked about the automation that's necessary for provisioning, managing and adapting a fleet of computers in a scalable way. **We called out that an important concept in today's IT world is to treat our infrastructure as code. This lets us manage our fleet of computers in a consistent, versionable, reliable and repeatable way.** To figure out how to get there, we've covered a lot of concepts related to configuration management, like how these tools use a domain specific language to help us clearly state what we want our system to look like after the tools have run. We've mentioned that the language is declarative because we declare our goals rather than detail the steps to get there, and most importantly the actions taken must be idempotent so that several runs of the same rules always lead to the same results. All along, we've been using Puppet as an example of how configuration management tools work. We looked into the puppet DSL syntax and checked out the most common resources: packages, files and services. We'll learn about other resources and other advanced techniques in future videos. But by now you should have a pretty good idea of what Puppet rules look like and how you can put into action the configuration management concepts that we discussed. With the concepts we've covered, you're probably starting to see how keeping your fleet of machines, whether they're virtual or physical, off of a pedestal is good practice. If something breaks, goes down or catches fire literally or figuratively, you can easily spin up a replacement because you know exactly what it's supposed to look like from the configuration and you can deploy it easily using the automation. In the next module, we'll check out how you can deploy Puppet in your infrastructure and look into some more advanced configuration management and change management techniques. Before that, you'll have the opportunity to try out fixing a system where the configuration management isn't doing what it's supposed to. You'll see what running the Puppet agent looks like in practice and find out what's wrong with the deployed rules, and then get the automation to behave as expected. Cool, right? Let's go for it.

## Deploying Puppet

### Applying Rules Locally

Up to now we've been getting to know Puppet syntax and understanding the different resources available. It's now time for the next step, trying out some Puppet rules on our local computer. In an earlier video, we called out that Puppet is usually deployed in a client-server architecture. But that's not the only way we can use Puppet. We can also use it as a stand-alone application run from the command line. This is common when testing new configurations. It can be the preferred configuration for complex setups where connecting to a master is no longer the best approach. When using a stand-alone Puppet, the same computer processes the facts, calculates the rules that need to be applied, and makes any necessary changes locally. So to get started with our Puppet deployment, let's first install Puppet and then we can start experimenting with running rules locally. In later videos, we'll check out how to create a client-server deployments. As we've called out, Puppet is available on a number of different platforms. We can either install it from the package management system available in the OS or download it from the official website. Both options work fine and the best one to choose will depend on our specific needs. For this exercise, we'll just go with the Puppet packages provided by the Ubuntu distribution. We'll do that by installing the Puppet master package using sudo apt install puppet-master.
We now have the package installed and can start trying out a few rules. We'll begin by creating the simplest possible Puppet file. We can make it more complex as we improve our deployments. For this example, we want to use Puppet to make sure that some useful tools for debugging problems are installed on each computer in our fleet. **To do this, we first have to create a file where we'll store the rules that we want to apply. In Puppet lingo, these files are called manifests and they must end with a.pp extension.** So we'll create a new file called tools.pp and in this file, we'll create a package resource. We'll start by managing the htop package which is a tool similar to top that can show us some extra information. We'll state that we want Puppet to ensure that we have this package present on our computer. Cool. That was simple. That's all we have to do. This resource will take care of installing the package for us. Let's save the file and try it out. But before actually applying the rules, we want to check that the command isn't present yet.
Htop isn't installed yet. Let's fix that by running our rules using **sudo puppet apply -v tools.pp**.
The -v flag tells Puppet that we want to get verbose output which will tell us what's going on while Puppet is applying the rules in the file that we pass to it. So here, Puppet first told us that it was loading the facts. Then, that it compiled a catalog. After that, it told us that it was applying the current configuration. Then, that it installed the package we requested. Finally, it let us know that it finished applying this catalog. You're probably wondering, **what's a catalog? We called out in an earlier video that after loading all facts for a computer, the server calculates which rules actually need to be applied.** For example, if a packet should only be installed when a certain condition is met, this condition is evaluated on the server side based on the gathered facts. The catalog is the list of rules that are generated for one specific computer once the server has evaluated all variables, conditionals, and functions. In this example, the catalog will be exactly the same as our code because the code didn't include any variables, functions, or conditionals. More complex sets of rules can lead to different catalogs depending on fact values. It's now time to check if our rules actually works. Let's try running the htop command again now that Puppet has installed it for us.
Yes, this time it worked. If our computer was misbehaving, we could now use this tool to get a better idea why. But fortunately, our computer's on its best behavior. So we'll exit now using q. Let's see what happens if we try to apply the Puppet rules again now that the package is installed.
Puppet's smart. It noticed that the package is already installed so it didn't try to install the package again. This means it applied the catalog much faster because nothing had to be changed. We've now seen how to write a Puppet resource in a manifest file and then use puppet apply to apply those rules to one computer. Up next, we'll check out how we can manage relationships between different Puppet resources and what that looks like when applied.

### Managing Resource Relationships
In our last video, we wrote a very simple manifest which we then applied locally. That was a great way to practice applying Puppet rules, but it was super-simple. Let's challenge ourselves with something a bit more tricky. The Puppet manifests that we use to manage computers in our fleet usually include a bunch of different resources that are related to each other. You're not going to configure a package that's not installed and you don't want to start a service until both the package and the configuration are in place. Puppets lets us control this with resource relationships. Let's check this out in an example. We have a file called ntp.pp, that has a bunch of resources related to the NUTS configuration like the one we've seen in an earlier video.
This time, on top of declaring the resources that we need to manage, we're also declaring a few relationships between them. We see that the configuration file requires the NTP package and the service requires the configuration file. This way, Puppet knows that before starting the service, the configuration file needs to be correctly set, and before sending the configuration file, the package needs to be installed. We're also declaring that the NTP service should be notified if the configuration file changes. That way, if we make additional changes to the contents of the configuration file in the future, the service will get reloaded with the new settings. If you look closely, you might notice that the resource types are written in lowercase, but relationships like require or notify use uppercase for the first letter of the resource. **This is part of Puppet syntax. We write resource types in lowercase when declaring them, but capitalize them when referring to them from another resource's attributes.** This sounds confusing right now, don't worry. It might take a while to wrap your head around it, but it will eventually click. Now, one last thing. At the bottom of the file, we have a call to include NTP. That's why we told Puppet that we want to apply the rules described in a class. For this example, we put the definition of the class and the call to include the class in the same file. Typically, the class is defined in one file and include it in another. We'll checkout examples for this in later videos. All right. Let's apply these rules locally.
Great. Our rules have run and in the verbose output, we can see that it did a bunch of things. First, it installed the package, then it checked that the configuration file needed to be updated and so it changed its contents. Finally, after changing the contents of the configuration, Puppet knew to restart the NTP service. We see here how our Puppet rules have translated into a few different actions. That's cool, but it's about to get even better. Let's make a change to the configuration file by editing the ntp.com file in this directory.
This is the configuration values by the NTP service. It's currently using a bunch of servers from ntp.org. But instead of those servers, we want to try out the NTP servers provided by Google. These are called time1.google.com, and then time2, time3, and time4.
Start transcript at 3 minutes 42 seconds
We've made the change, we'll save with :WQ and then rerun our Puppet rules with the new configuration file.
Awesome. Puppet updated the configuration file with the new contents and then refresh the service, so it loaded the config. Success. In this video, we've seen how we can apply a Puppet manifests that includes a class with a bunch of resources. We grouped all of the information related to the NTP service in a manifest specific to it, which is common practice when dealing with Puppet rules. We want to keep related operations together and separate things that are unrelated. Up next, we'll look into how we can do that using Puppet modules.

### Organizing Your Puppet Modules

In any configuration management deployment, there's usually a lot of different things to manage. We might want to install some packages, copy some configuration files, start some services, schedule some periodic tasks, make sure some users and groups are created and have access to specific devices, and maybe execute a few commands that aren't provided by existing resources. On top of that, there might be different configurations applied to the different computers in the fleet. For example, workstations and laptops might include resources that aren't used on servers. Each distinct type of server will need its own specific setup. There's a lot of different things to manage. We need to organize all these resources and information in a way that helps us maintain them long-term. This means grouping related resources, giving the groups good names, and making sure that the organization will make sense to new users. In puppet, we organize our manifests into modules. **A module is a collection of manifests and associated data.** We can put any resource we want into a module, but to keep our configuration management organized, we'll group things together under a sensible topic. For example, we could have a module for everything related to monitoring the computer's health, another one for setting up the network stack, and yet another one for configuring a web serving application. So the module ship the manifest in the associated data, but how is this organized? 
**All manifests gets stored in a directory called manifests. The rest of the data is stored in different directories depending on what it does.** 
**The files directory includes files that are copied into the client machines without any changes, like the ntp.conf file that we saw in our last video.** 
**The template's directory includes files that are preprocessed before they've been copied into the client machines.** These templates can include values that get replaced after calculating the manifests, or sections that are only present if certain conditions are valid. There's a bunch more directories that can be part of a module depending on what exactly the module does. But you don't need to worry about these when creating your first puppet module. You can start with the simple module that just has one manifest in the Manifest directory. This file should be called init.pp and it should define a class with the same name as the module that you're creating. Then any files that your rules use need to be stored in the files or templates directories depending on whether you copy them directly or need to preprocess them. For example, this is how the NTP class that we saw in our last video looks like when turned into a module.

There's an init.pp file, which contains the NTP classes that we saw before, and the ntp.conf file that gets deployed onto the machine is now stored in the files directory. Modules like these can look pretty much the same no matter who's using them. That's why over time, system administrators using puppet have shared the modules they've written, letting others use the same rules. By now, there's a large collection of prepackaged modules that are shipped and ready to use. If one of those modules does what we want, we can just install it on our Puppet server and use it in our deployments. Let's install the Apache module provided by Puppet Labs to check out how this works.

We've installed the module. Let's have a quick look at its contents. First, we'll change into the directory where the module files are stored and list its contents.
We see the files, manifests, and templates directories that we mentioned. On top of that, there's a lib directory that adds functions and fact to the ones already shipped by puppet. The metadata.json file includes some additional data about the module we just installed, like which versions of which operating systems it's compatible with. Let's peek into the manifest directory.

That's a lot of files, like how we split the different things that we want to manage into separate modules. We can also split each separate functionality that we want to configure into separate manifests. This helps us organize our code when we make changes to it, and to see how this directory also contains its own init.pp. As we called out, this manifest is special. It needs to always be present because it's the first one that's read by puppet when a module gets included. So how do we include a module like this one? It's pretty easy. Let's create a manifest file that includes the module we've just installed.
Here, we're telling Puppet to include the Apache module. The double colon before the module name, let's puppet know that this is a global module. Let's save this file now and apply it using Puppet apply like we did before.
Start transcript at 5 minutes 50 seconds
Our manifest was super-simple, it just include the Apache module. But by including the module, we got puppet to apply all the rules run by default in the module. We now have an Apache server configured and ready to run on this machine. We've just seen how we can organize our code in modules and how we can even use modules provided by other teams so we don't have to reinvent the wheel. Up next, there's a reading with pointers to more information, followed by a quick quiz. After that, meet me over in the next video, where we'll check out what we need to do to deploy our rules to more machines.

**A module is an easy way to organize our configuration management tools.**

### Links

Check out the following links for more information:

https://puppet.com/docs/puppet/latest/style_guide.html
https://puppet.com/docs/puppetserver/latest/install_from_packages.html

### Puppet Nodes

When managing fleets of computers, we usually want some rules to apply to every computer, and other rules to apply only to a subset of systems. Let's say you're managing all your servers with Puppet. You might want to install a basic set of tools on all of them, but only install the packages for serving web pages in your web servers. And only install the packages for sending and receiving email in your mail servers. There's a bunch of different ways that we can do this. In an earlier video, we saw how to conditionally apply some rules using facts from the machines. Another way to apply different rules to different systems is to use separate node definitions. In Puppet terminology, a node is any system where we can run a Puppet agent. It could be a physical workstation, a server, a virtual machine, or even a network router, as long as it has a Puppet agent and can apply the given rules. So we can set up Puppet to give some basic rules to all the nodes, but then apply some specific rules to the nodes that we want to be different. Let's check out an example of how this could look. When setting up Puppet, we usually have a default node definition that lists the classes that should be included for all the nodes. For example, it could look something like this.

Here, the default node is including two classes, the sudo class and the ntp class. For the ntp class, we're setting an additional servers parameter that lists the servers we can use to get the network time.

As you can see here, when defining a node, you can include a class by just using its name if there's no additional settings, or include the class and set additional parameters if necessary. All right, that's the default node, so it will apply to computers in the fleet by default. What if you want some settings to only apply to some specific nodes? You can do that by adding more node definitions that list the classes that you want them to include, like this. **We can see here that specific nodes in the fleet are identified by their FQDNs, or fully qualified domain names.** In this case, we have the node definition for a host called webserver.example.com. For this node, we're including the same sudo and ntp classes as before, and we're adding the apache class on top. We're listing the same classes because the classes included in the default node definition are only applied to the nodes that don't have an explicit entry. In other words, when a node requests which rules it should apply, Puppet will look at the node definitions, figure out which one matches the node's FQDN, and then give only those rules. To avoid repeating the inclusion of all the common classes, we might define a base class that does the work of including all the classes that are common to all node types. Now, where's this information stored? The node definitions are typically stored in a file called site.pp, which isn't part of any module. Instead, it just defines what classes will be included for what nodes. This is another step towards helping us organize our code in a way that makes it easier to maintain. Up next, we'll look into the infrastructure used by Puppet to verify if a node really has the name that it claims to have.

**Different kinds of nodes are defined, allowing different sets of rule catalogs to apply to different types of machines.**

### Puppet's Certificate Infrastructure

We've called that a few times that in typical Puppet deployments, all managed machines and the fleet connect to a Puppet server. **The client send their facts to the server, and the server then processes the manifests, generates the corresponding catalog, and sends it back to the clients who apply it locally.** In our last video, we mentioned that we can apply different rules to different nodes depending on their names. The client send their name to the server when they connect, but how can the server trust that a client is really who he claims to be? It's a dangerous world out there. Well, this is a complex subject that touches on some important security concepts. We'll do a quick rundown here. If you're interested in learning more, you might want to check out the security course in the Google IT support professional certificate program led by my colleague, Gian, who explains it in more detail. **Puppet uses public key infrastructure, or PKI, to establish secure connections between the server and the clients.** There's a bunch of different types of public key technologies. The one used by Puppet is secure sockets layer or SSL. This is the same technology used for encrypting transmissions over HTTPS. The clients use this infrastructure to check the server's identity, and the server uses it to check the client's identity, and all communication is done over an encrypted channel that uses these identities so it can't be intercepted by other parties. So how does this work? Each machine involved has a pair of keys related to each other, a private key and a public key. The private key is secret, only known to that specific machine, the public key is shared with other machines involved. Machines can then use the standardized process to validate the identity of any other machine. The sender signs a message using the private key and the receiver validates the signature using the corresponding public key. Okay. But how do machines know which public keys to trust? This is where a certificate authority, or CA comes in. The CA verifies the identity of the machine and then creates a certificate stating that the public key goes with that machine. After that, other machines can rely on that certificate to know that they can trust the public key, since it means the machine's identity has been verified. Puppet comes with its own certificate authority, which can be used to create certificates for each clients. So you can use that one, or if your company already has a CA that validates the identity of the machines in your fleet, you can integrate it with Puppet, so you only validate the identities once. Now, let's assume you're using the baked-in certificate infrastructure and dive into how this process works. When a node checks into the Puppet master for the first time, it requests the certificate. The Puppet master looks at this request and if it can verify the nodes identity, it creates a certificate for that node. The system administrator can check the identity manually or use a process that does this automatically using additional information about the machines to verify their identity. When the agent node picks up this certificate, it knows it can trust the Puppet master, and the node can use the certificate from then on to identify itself when requesting a catalog. You might be wondering, why do we care so much about the identity of the nodes? There's a bunch of reasons. First, Puppet rules can sometimes include confidential information that you don't want to fall in the wrong hands. But even if none of the rules hold confidential info, you want to be sure that the machine you're setting up as your web server really is your web server and not a rogue machine that just claims to have the same name. All sorts of things could go wrong if random computers start popping up in your network with the wrong settings. If you're creating a test deployment to try out how Puppet rules get applied, and so you're only managing tests machines, you can configure Puppet to automatically sign all requests, but you should never do this for real computers being used by real users. Remember that it's better to be safe than sorry. So always take the time to authenticate your machines. When starting out with Puppet, it's common to use the manual signing approach. In this case, when the node connects to the master, it will generate a certificate request, which we'll go into a queue in the Puppet master machine. You'll then need to verify that the machine's identity is correct and the baked-in CA will issue the corresponding certificate. If your fleet is large, this manual approach won't really work. Instead, you'll want to write a script that verifies the identity of the machines automatically for you. One way to do this is by copying a unique piece of information into the machines when they get provisioned and then use this pre-shared data as part of the certificate request. That way, your script can verify that the machines are who they claim to be without involving any humans. Great, you now have a broad idea of the infrastructure that Puppet uses to identify the nodes when they connect to the master. Up next, we'll see what the typical Puppet setup using a separate Puppet server and client looks like in practice.

### Setting up Puppet Clients and Servers

sudo puppet config --section master set autosign true
ssh webserver

sudo puppet config set server ubuntu.example.com

sudo puppet agent -v --test

sudo systemctl enable puppet

sudo systemctl start puppet 

sudo systemctl status puppet

We're now ready to see a Puppet deployment in action. We've already installed the Puppet master package on this computer, so we'll use it as the master. Since this is a test deployment to demonstrate Puppet, we'll configure it to automatically sign the certificate requests of the nodes we add. But remember, if we were deploying this to real computers, we'd have to manually sign the requests or implement a proper validating script. We'll do this by calling the Puppet command with the config parameter, and then saying that in this section master we want to set auto sign to true. All right. With that, we can connect to the client that we want to manage using Puppet. We'll connect using SSH to a machine called web server. On this machine, we'll install the Puppet client which is shipped by the Puppet package.

Nice. We have the Puppet agent installed. Now we need to configure it to talk to the Puppet server that we're running on the other machine. To do that, we'll use Puppet config like before but this time we'll tell it that we want to set the server to ubuntu.example.com.

Great. Now that we've configured the server, we can test the connection to the Puppet master by using the Puppet agent command passing dash v as before to get verbose output, and dash dash test to do a test run. As usual, Puppet tells us everything it did. It first created an SSL key for the machine. It then read a bunch of information from the machine and used this to create a certificate request. The agent shows us the fingerprint of the certificate requested. If we were using manual signing, we could use this fingerprint to verify that the request and the server matches the one generated on the machine. The certificate was then generated on our puppet master. We don't see any entries for that because it happened on the other computer. But we see that this computer received a certificate and stored it locally. Once the certificate exchange completed, the agent retrieved all the information from the machine and sent it to the master. In exchange, it got back a catalog and applied it. The catalog applied almost immediately because we haven't actually configured any rules to be applied to our clients. We should go ahead and do that now. We'll go back to our Puppet master and create a couple of node definitions. **As we called out, node definitions are stored in a manifest file called site.pp** which is stored at the root of the nodes environment. We'll talk more about environments in a later video. For now, we just need to know that our client is trying to access the production environment. So the file that we need to create will be located in slash etc puppet code environments production manifests, and it will be called site.pp.

In this file, we'll create a couple of node definitions. We want to install Apache in our web server, so we'll create a node definition for the web server with the Apache class and node parameters for now, and we'll also add a default node definition. We'll keep it empty for now. We can add more classes in the future. All right. With that, we have our very basic node definition. We can now save this and run the Puppet agent on our web server machine again.

This time, the Puppet agent connected to the Puppet master and got a catalog that told it to install and configure the Apache package. This included setting up a bunch of different services. Up to now, we've been doing manual runs of the Puppet agent for testing purposes. Now that we know it's working fine, we want to keep Puppet running automatically. That way, if we make changes to the configuration, clients will automatically apply those changes without us having to do any manual steps. So to do that, we'll use the system CTL command, which lets us control the services that are enabled when the machine starts and those that are currently running. So we'll first tell the system CTL to enable the puppet service so that the agent gets started whenever the machine reboots, and then we'll tell system CTL to start the puppet service so that it starts running. Last step, we'll ask systems CTL for the status of the Puppet service to check that it's actually running. Awesome. That worked. The Puppet agent will keep regularly checking in with the master and ask if there are any changes that need to be applied to the machine. With that, you've seen Puppet in action using the server client model. We use the configuration we set in the Puppet master to manage the installation and configuration of software in our web server, and we set up the Puppet agent in the web server to keep running so that the configuration stays up to date. We've only seen the very basics of how to configure Puppet, but this can already give you an idea of how powerful configuration management can be. Pretty exciting, right? Up next, we've gathered more info on how to do the client-server set-up and after that, a quick quiz to check that everything is still making sense.

**The Certificate Authority creates an SSL key for the agent machine and creates a certificate request.**

### Links

Check out the following link for more information:

http://www.masterzen.fr/2010/11/14/puppet-ssl-explained/

