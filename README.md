# Configuration-Management-and-the-Cloud
Writing of lectures

## Automating with Configuration Management

### What is scale?

In this course we'll focus on making our work scale. So what do we mean when we talk about scale? **Being able to scale what we do means that we can keep achieving larger impacts with the same amount of effort when a system scales**. Well an increase in the amount of work it needs to do can be accommodated by an increase in capacity. For example, if the web application your company provides is scalable, that it can handle an increase in the number of people using it by adding more servers to serve requests. In short, a scalable system is a flexible one. Adding more computers to the pool of servers that are serving the website can be a very simple or very hard operation depending on how your infrastructure is set up. To figure out how scalable your current setup is, you can ask yourself questions like will adding more servers increase the capacity of the service? How are new servers prepared, installed, and configured? How quickly can you set up new computers to get them ready to be used? Could you deploy a hundred servers with the same IT team that you have today? Or would you need to hire more people to get it done faster? Would all the deployed servers be configured exactly the same way? Scaling isn't just about website serving content of course. If your company is rapidly hiring a lot of new employees, you'll need to have an onboarding process that can scale as needed. And as you keep adding new computers to the network, you'll need to make sure that your system administration process can scale to the growing needs of the company. This can include tasks like a applying the latest security policies and patches while making sure users' needs still get addressed all while more and more users join the network without new support staff to back you up. If making this happen sounds a bit like magic right now, remember that we're here to share the secret ingredient with you, automation. **Automation is an essential tool for keeping up with the infrastructure needs of a growing business.** By using the right automation tools, we can get a lot more done in the same amount of time. For example, we could deploy a whole new server by running a single command and letting the automation take care of the rest. We could also create a batch of user accounts with all the necessary permissions based on data already stored in the database, eliminating all human interaction. Automation is what lets us scale. It allows a small IT team to be in charge of hundreds or even thousands of computers. Okay, so what does that look like in practice? There's a bunch of different tools that we can use to achieve this. Up next, we'll talk about a type of tool called configuration management that can help us automate how we manage the computers in our fleets.

### What is configuration management?

Imagine your team is in charge of setting up a new server. This could be a physical computer running close to you or a virtual machine running somewhere in the cloud. To get things moving, the team installs the operating system, configures some applications and services, sets up the networking stack, and when everything is ready, puts the server into use. By manually deploying the installation and configuring the computer, we see that we're using unmanaged configuration. When we say configuration here, we're talking about everything from the current operating system and the applications installed to any necessary configuration files or policies, including anything else that's relevant for the server to do its job. When you work in IT, you're generally in charge of the configuration of a lot of different devices, not just servers. Network routers printers and even smart home devices can have configuration that we can control. For example, a network switch might use a config file to set up each of its ports. All right, so now we know what we mean when we talk about configuration. We said that manually deploying a server means that the configuration is unmanaged. **So what would it mean for the configuration to be managed? It means using a configuration management system to handle all of the configuration of the devices in your fleet, also known as nodes.** There's a bunch of different tools available depending on the devices and services involved. Typically you'll define a set of rules that have to be applied to the nodes you want to manage and then have a process that ensures that those settings are true on each of the nodes. At a small scale, unmanaged configurations seem inexpensive. If you only manage a handful of servers, you might be able to get away with doing that without the help of automation. You could log into each device and make changes by hand when necessary. And when your company needs a new database server, you might just go ahead and manually install the OS and the database software into a spare computer. But this approach doesn't always scale well. The more servers that you need to deploy, the more time it will take you to do it manually. And when things go wrong, and they often do, it can take a lot of time to recover and have the servers back online. Configuration management systems aim to solve this scaling problem. By managing the configuration of a fleet with a system like this, large deployments become easier to work with because the system will deploy the configuration automatically no matter how many devices you're managing. When you use configuration management and you need to make a change in one or more computers, you don't manually connect to each computer to perform operations on it. Instead, you edit the configuration management rules and then let the automation apply those rules in the affected machines. This way the changes you make to a system or group of systems are done in a systematic, repeatable way. Being repeatable is important because it means that the results will be the same on all the devices. A configuration management tool can take the rules you define and apply them to the systems that it manages, making changes efficient and consistent. Configuration management systems often also have some form of automatic error correction built in so that they can recover from certain types of errors all by themselves. For example, say you found that some application that was being used widely in your company was configured to be very insecure. You can add rules to your configuration management system to improve the settings on all computers. And this won't just apply the more secure settings once. It will continue to monitor the configuration going forward. If a user changes the settings on their machine, the configuration management tooling will detect this change and reapply the settings you defined in code. How cool is that? **There are lots of configuration management systems available in the IT industry today. Some popular systems include Puppet, Chef, Ansible, and CFEngine.** These tools can be used to manage locally hosted infrastructure. Think bare metal or virtual machines, like the laptops or work stations that employees use at a company. Many also have some kind of Cloud integration allowing them to manage resources in Cloud environments like Amazon EC2, Microsoft Azure, or the Google Cloud platform, and the list doesn't stop there. There are some platform specific tools, like SCCM and Group Policy for Windows. These tools can be very useful in some specific environments, even when they aren't as flexible as the others. For this course, we've chosen to focus on Puppet because it's the current industry standard for configuration management. Keep in mind though that selecting a configuration management system is a lot like deciding on a programming language or version control system. You should pick the one that best fits your needs and adapt accordingly, if necessary. Each has its own strengths and weaknesses. So a little research beforehand can help you decide which system is best suited for your particular infrastructure needs. There are a lot of tools out there. So be sure to check them out. Up next, we'll discuss how we can make the most out of our configuration management system using the infrastructure as code paradigm.

### What is infrastructure as code?

We've called out that when we use a configuration management system, we write rules that describe how the computers in our fleet should be configured. These rules are then executed by the automation, to make the computers match our desired state. This means that we can model the behavior of our IT infrastructure in files that can be processed by automatic tools. These files can then be tracked in a version control system. Remember, version control systems help us keep track of all changes done to the files, helping answer questions like who, when, and why. More importantly, they're super-useful when we need to revert changes. This can be especially helpful if a change turns out to be problematic. **The paradigm of storing all the configuration for the managed devices in version controlled files is known as Infrastructure as Code or IaC.** In other words, we see that we're using Infrastructure as Code when all of the configuration necessary to deploy and manage a node in the infrastructure is stored in version control. This is then combined with automatic tooling to actually get the nodes provisioned and managed. If you have all the details of your Infrastructure properly stored in the system, you can very quickly deploy a new device if something breaks down. Simply get a new machine, either virtual or physical, use the automation to deploy the necessary configuration, and you're done. **The principals of Infrastructure as Code are commonly applied in cloud computing environments, where machines are treated like interchangeable resources, instead of individual computers. This principle is also known as treating your computers as cattle instead of pets because you care for them as a group rather than individually.** Apologies to anyone with a pet cow. This concept isn't just for managing computers in huge data centers or globe spanning infrastructures, it can work for anything; from servers to laptops, or even workstations in a small IT department. Even if your company only has a single computer working as the mail server, you can still benefit from storing all the configuration needed to set it up in a configuration management system. That way if the server ever stops working, you can deploy a replacement very quickly by simply applying the rules that configure the mail server to the new computer. One valuable benefit of this process is that the configuration applied to the device doesn't depend on a human remembering to follow all the necessary steps. Rest assured, silly human, the result will always be the same, making the deployment consistent. As mentioned, having Infrastructure as Code means that we can also apply the benefits of the version control system or VCS to your infrastructure. Since the configuration of our computers is stored in files, those files can be added to a VCS. This has all the benefits that version control systems bring. It gives us an audit trail of changes, it lets us quickly rollback if a change was wrong, it lets others reviewed our code to catch errors and distribute knowledge, it improves collaboration with the rest of the team, and it lets us easily check out the state of our infrastructure by looking at the rules that are committed. Not too shabby. I personally think this is one of the coolest things about IaC. The ability to easily see what configuration changes were made and roll back to a known good state is super important. It can make a big difference in quickly recovering from an outage, especially since changing the contents of the configuration file can be as dangerous as updating the version of an application. I've had my fair share of outages caused by an innocent-looking change with unintended side effects. But storing all the infrastructure in a version control system lets me quickly roll back to a previously known good version so that the outage length can be minimized. On top of that, having the rules stored in files means that we can also run automated tests on them. It's much better to find out in a test that a configuration file has a typo in it than to find out from our users. In a complex or large environment, treating your IT Infrastructure as Code can help you deploy a flexible scalable system. A configuration management system can help you manage that code by providing a platform to maintain and provision that infrastructure in an automated way. Having your infrastructure stored as code means that you can automatically deploy your infrastructure with very little overhead. If you need to move it to a different location, it can be deployed, de-provisioned, and redeployed at scale in a different locale with minimal code level changes. **To sum all of this up, managing your Infrastructure as Code it means that your fleet of nodes are consistent, versioned, reliable, and repeatable.** Instead of being seen as precious or unique, machines are treated as replaceable resources that can be deployed on-demand through the automation. Any infrastructure that claims to be scalable must be able to handle the capacity requirements of growth. Performing an action like adding more servers to handle an increase in requests is just a possible first step. There are other things that we might need to take into account, such as the amount of traffic that network can handle or the load on the back end servers like databases. Viewing your infrastructure in this way helps your IT team adapt and stay flexible. The technology industry is constantly changing and evolving. Automation and configuration management can help you embrace that change instead of avoiding it. Before diving into concrete examples of what this looks like, the first practice quiz of the course is coming up. These quizzes act as check-in points to help you make sure all the concepts covered in the videos are making sense. See you on the other side.

### What is Puppet?

As we called out a couple of times already, in this course, we'll be learning how to apply basic configuration management concepts by using Puppet. **Puppet is the current industry standard for managing the configuration of computers in a fleet of machines.** Part of the reason why Puppet is so popular is that it's a cross-platform tool that's been around for a while. It's an open source project that was created in 2005, and it's gone through several different versions. As it's evolved, the tool has incorporated feedback from its users to make it more and more useful. The latest available version at the time this Google course went live is Puppet 6, which came out in late 2018. **We typically deploy puppet using a client-server architecture. The client is known as the Puppet agent, and the service is known as the Puppet master.** When using this model, the agent connects to the master and sends a bunch of facts that describe the computer to the master. The master then processes this information, generates the list of rules that need to be applied on the device, and sends this list back to the agent. The agent is then in charge of making any necessary changes on the computer. **Puppet is a cross-platform application available for all Linux distributions, Windows, and Mac OS. This means that you can use the same puppet rules for managing a range of different computers.** What are these rules that we keep talking about? Let's check out a very simple example. This block is saying that the package 'sudo' should be present on every computer where the rule gets applied. If this rule is applied on 100 computers, it would automatically install the package in all of them. This is a small and simple block but can already give us a basic impression of how rules are written in puppet. Don't worry too much about the syntax now, we'll look into what each piece means in future videos. There are various installation tools available depending on the type of operating system. Puppet will determine the type of operating system being used and select the right tool to perform the package installation. **On Linux distributions, there are several package management systems like APT, Yum, and DNF.** Puppet will also determine which package manager should be used to install the package. On Mac OS, there's a few different available providers depending on where the package is coming from. **The Apple Provider is used for packages that are part of the OS, while the MacPorts provider is used for packages that come from the MacPorts Project. For Windows, we'll need to add an extra attribute to our rule, stating where the installer file is located on the local desk or a network mounted resource.** Puppet will then execute the installer and make sure that it finishes successfully. If you use Chocolatey to manage your windows packages, you can add an extra Chocolatey provider to Puppet to support that. We'll add a link to more information about this in our next reading. **Using rules like this one, we can get puppet to do a lot more than just install packages for us. We can add, remove, or modify configuration files stored in the system, or change registry entries on Windows. We can also enable, disable, start, or stop the services that run on our computer.** We can configure crone jobs, the scheduled tasks, add, remove, or modify Users and Groups or even execute external commands, if that's what we need. There's a lot to say about puppet. We won't go into absolutely every detail, but we'll cover the most important concepts in this course. The goal is to get you started with what you need to know about configuration management in general and puppet in particular. We'll also give you pointers to find out more information on your own. Up next, we'll check out the different resources we can use to define our rules.

### Puppet Resources

In our last video, we saw an example that installed the pseudo package in a computer. To do that, our example used the package keyword declaring a package resource. **In puppet, resources are the basic unit for modeling the configuration that we want to manage. In other words, each resource specifies one configuration that we're trying to manage, like a service, a package, or a file.** Let's look at another example. In this case, we're defining a file resource. This resource type is used for managing files and directories. In this case, it's a very simple rule that ensures that etc/sysctl.d exists and is a directory. Let's talk a little bit about syntax. In both our last example and this one we could see that when declaring a resource in puppet, we write them in a block that starts with the resource type ,in this case File. The configuration of the resource is then written inside a block of curly braces. Right after the opening curly brace, we have the title of the resource, followed by a colon. After the colon come the attributes that we want to set for the resource. In this example, we're once again setting the insurer attribute with directory as the value, but we could set other attributes too >> Let's check out a different file resource. In this example, we're using a file resource to configure the contents of etc/timezone, a file, that's used in some Linux distributions to determine the time zone of the computer. This resource has three attributes. First, we explicitly say that this will be a file instead of a directory or a symlink then we set the contents of the file to the UTC time zone. Finally, we set the replace attribute to true, which means that the contents of the file will be replaced even if the file already exists. We've now seen a couple examples of what we can do with the file resource. There are a lot more attributes that we could set, like file permissions the file owner, or the file modification time.
We've included a link to the official documentation in the next reading where you can find all the possible attributes that can be set for each resource. How do these rules turn into changes in our computers? When we declare a resource in our puppet rules. We're defining the desired state of that resource in the system. The puppet agent then turns the desired state into reality using providers.
**The provider used will depend on the resource defined and the environment where the agent is running.** Puppet will normally detect this automatically without us having to do anything special. When the puppet agent processes a resource, it first decides which provider it needs to use, then passes along the attributes that we configured in the resource to that provider. **The code of each provider is in charge of making our computer reflect the state requested in the resource.** In these examples, We've looked at one resource at a time. Up next, we'll see how we can combine a bunch of resources into more complex puppet classes.

### Puppet Classes

In the examples of Puppet code that we've seen so far, we've declared classes that contain one resource. You might have wondered what those classes were for. We use these classes to collect the resources that are needed to achieve a goal in a single place. For example, you could have a class that installs a package, sets the contents of a configuration file, and starts the service provided by that package. Let's check out an example like that. In this case, we have a class with three resources, a package, a file, and a service. All of them are related to the Network Time Protocol, or NTP, the mechanism our computers use to synchronize the clocks. Our rules are making sure that the NTP package is always upgraded to the latest version. We're setting the contents of the configuration file using the source attribute, which means that the agent will read the required contents from the specified location. And we're saying that we want the NTP service to be enabled and running. By grouping all of the resources related to NTP in the same class, we only need a quick glance to understand how the service is configured and how it's supposed to work. This would make it easier to make changes in the future since we have all the related resources together. It makes sense to use this technique whenever we want to group related resources. For example, you could have a class grouping all resources related to managing log files, or configuring the time zone, or handling temporary files and directories. You could also have classes that group all the settings related to your web serving software, your email infrastructure, or even your company's firewall. We're just getting started with Puppet's basic resources and seeing how they can be applied. In further videos, we'll be learning a lot more about common practices when using configuration management tools. But before jumping into that, we've put together a reading with more information about Puppet syntax, resources and links to the official reference. Then we've got a quick quiz to check that everything is making sense.
**By grouping related resources together, we can more easily understand the configuration and make changes in the future.**

### Links 

Check out the following links for more information:

https://puppet.com/docs/puppet/latest/lang_resources.html
https://puppet.com/blog/deploy-packages-across-your-windows-estate-with-bolt-and-chocolatey/

### What are domain-specific languages?

Up until now, we've seen examples of very simple Puppet rules they just define whiner more resources. **These resources are the building blocks of Puppet rules, but we can do much more complex operations using Puppet's domain specific language or DSL.** **Typical programming languages like Python, Ruby, Java or Go are general purpose languages that can be used to write lots of different applications with different goals and use cases.** On the flip side, **a domain specific language is a programming language that's more limited in scope.** Learning a domain-specific language is usually much faster and easier than learning a general purpose programming language because there's a lot less to cover. You don't need to learn as much syntax or understand as many keywords or taking to account a lot of overhead in general. In the case of Puppet, the DSL is limited to operations related to when and how to apply configuration management rules to our devices. For example, we can use the mechanisms provided by the DSL to set different values on laptops or desktop computers, or to install some specific packages only on the company's web servers. On top of the basic resource types that we already checked out, Puppet's DSL includes variables, conditional statements, and functions. Using them, we can apply different resources or set attributes to different values depending on some conditions. Before we jump into an example of what that looks like, let's talk a bit about **Puppet facts. Facts are variables that represent the characteristics of the system.** __When the Puppet agent runs, it calls a program called factor which analyzes the current system, storing the information it gathers in these facts.__ Once it's done, it sends the values for these facts to the server, which uses them to calculate the rules that should be applied. Puppet comes with a bunch of baked-in core facts that store useful information about the system like what the current OS is, how much memory the computer has whether it's a virtual machine or not or what the current IP address is. If the information we need to make a decision isn't available through one of these facts, we can also write a script that checks for the information and turns it into our own custom fact. Let's check out an example of a piece of Puppet code that makes use of one of the built-in facts. **This piece of code is using the is-virtual fact together with a conditional statement to decide whether the smartmontools package should be installed or purged. This package is used for monitoring the state of hard drives using smart.** So it's useful to have it installed in physical machines, but it doesn't make much sense to install it in our virtual machines. We can see several of the characteristics of Puppets domain specific language in this block. So let's spend a little time looking at all of the elements of syntax here. First, facts is a variable. All variable names are preceded by a dollar sign in Puppet's DSL. **In particular, the facts variable is what's known as a hash in the Puppet DSL, which is equivalent to a dictionary in Python.** This means that we can access the different elements in the hash using their keys. In this case, we're accessing the value associated to the is virtual key. Second, we see how we can write a conditional statement using if else, enclosing each block of the conditional with curly braces. Finally, each conditional block contains a package resource. We've seen resources before, but we haven't looked at the syntax in detail. So let's do that now. Every resource starts with the type of resource being defined. In this case, package and the contents of the resource are then enclosed in curly braces. Inside the resource definition, the first line contains the title followed by a colon. Any lines after that are attributes that are being set. We use equals greater than to assign values to the attributes and then each attribute ends with a comma. We've now covered a large chunk of puppet's DSL syntax. If you look back to what it was like to learn your first programming language, you'll probably notice how much less syntax there is to learn here. That's typical of the domain specific languages used by configuration management tools. While each tool uses their own DSL, they're usually very simple and can be learned very quickly. Up next, we'll talk about a few other principles behind most configuration management tools. Whenever you're ready, let's dive in.
**A fact is a hash that stores information about the details of a particular system.**

### The Driving Principles of Configuration Management

Up to now, we've seen a few examples of what Puppet rules look like, including a bunch of different resources and even a conditional expression. You might have noticed that in all the examples we've checked out, we were never telling the computer the steps it should follow in order to do what we wanted. Instead, we were just declaring the end goal that we wanted to achieve, like going to a drive-through and ordering a burger, we didn't make it, but there it is. **The providers that we mentioned earlier lake apt and yum are the ones in charge of turning our goals into whatever actions are necessary.** We say that Puppet uses a **declarative language** because we declare the state that we want to achieve rather than the steps to get there. **Traditional languages like Python or C are called procedural because we write out the procedure that the computer needs to follow to reach our desired goal.** Coming from a procedural language like Python, it might take some time to get used to writing declarative code like the ones used for Puppet, and that's okay. Just remember that when it comes to configuration management, it makes sense to simply state what the configuration should be, not what the computer should do to get there. Say you're using a resource to declare that you want a package installed, you don't care what commands a computer has to run you install it, you only care that after the configuration management tool has run, the package is installed. **Another important aspect of configuration management is that operations should be idempotent. In this context, an idempotent action can be performed over and over again without changing the system after the first time the action was performed, and with no unintended side effects.** Let's check this out with an example of a file resource. This resource ensures that the /etc/issue file has a set of permissions and a specific line in it. Fulfilling this requirement is an idempotent operation. If the file already exists and has the desired content, then Puppet will understand that no action has to be taken. If the file doesn't exist, then puppet will create it. If the contents or permissions don't match, Puppet will fix them. No matter how many times the agent applies the rule, the end result is that this file will have the requested contents and permissions. **Idempotency is a valuable property of any piece of automation. If a script is idempotent, it means that it can fail halfway through its task and be run again without problematic consequences.** Say you're running your configuration management system to setup a new server. Unfortunately, the setup fails because you forgot to add a second disk to the computer and the configuration required two disks. If your automation is idempotent, you can add the missing disk and then have the system pick up from where it left off. Most Puppet resources provide idempotent actions, and we can rest assured that two runs of the same set of rules will lead to the same end result. **An exception to this is the exec resource, which runs commands for us.** The actions taken by the exec resource might not be idempotent since a command might modify the system each time it's executed. To understand this, let's check out what happens when we execute a command that moves a file on our computer. First, we'll check that the example.txt file is here, and then we'll move it to the desktop directory.
This works fine now, but what happens if we run the exact same command again after it's been executed once? **We receive an error because the file is no longer in the same place. In other words, this was not an idempotent action, as executing the same action twice produced a different result and the unintended side effect of an error.** If we were running this inside Puppet, this would cause our Puppet run to finish with an error. So if we need to use the exec resource to run a command for us, we need to be careful to ensure that the action is idempotent. **We could do that for example by using the onlyif attribute like this. Using the onlyif attribute, we specified that this command should be executed only if the file that we want to move exists. This means that the file will be moved if it exists and nothing will happen if it doesn't.** By adding this conditional, we've taken an action that's not idempotent and turned it into an idempotent one. Another important aspect of how configuration management works is the **test and repair paradigm. This means that actions are taken only when they are necessary to achieve a goal.** Puppet will first test to see if the resource being managed like a file or a package, actually needs to be modified. If the file exists in the place we want it to, no action needs to be taken. If a package is already installed, there's no need to install it again. This avoids wasting time doing actions that aren't needed. Finally, another **important characteristic is that Puppet is stateless, this means that there's no state being kept between runs of the agent.** Each Puppet run is independent of the previous one, and the next one. **Each time the puppet agent runs, it collects the current facts. The Puppet master generates the rules based just on those facts, and then the agent applies them as necessary.** We're just getting started with what configuration management is and what it looks like in Puppet. But hopefully, you're starting to see how understanding these basic concepts and how turning them into practical rules can help you manage a small army of computers. Up next, there's a reading with links to more information about the concepts we've covered followed by a quick quiz. You've got this.

### Links 

Check out the following links for more information:

https://en.wikipedia.org/wiki/Domain-specific_language
http://radar.oreilly.com/2015/04/the-puppet-design-philosophy.html


### Module 1 Wrap Up: Automating with Configuration Management

We started our journey talking about what would happen if you needed to upgrade a package in a fleet of 1,000 different servers. If you've never heard about configuration management before, an upgrade like that probably seemed like a super long and boring task, right? But now you know that there's a bunch of tools you can use to make your life much easier when making large-scale changes like that one. We've talked about the automation that's necessary for provisioning, managing and adapting a fleet of computers in a scalable way. **We called out that an important concept in today's IT world is to treat our infrastructure as code. This lets us manage our fleet of computers in a consistent, versionable, reliable and repeatable way.** To figure out how to get there, we've covered a lot of concepts related to configuration management, like how these tools use a domain specific language to help us clearly state what we want our system to look like after the tools have run. We've mentioned that the language is declarative because we declare our goals rather than detail the steps to get there, and most importantly the actions taken must be idempotent so that several runs of the same rules always lead to the same results. All along, we've been using Puppet as an example of how configuration management tools work. We looked into the puppet DSL syntax and checked out the most common resources: packages, files and services. We'll learn about other resources and other advanced techniques in future videos. But by now you should have a pretty good idea of what Puppet rules look like and how you can put into action the configuration management concepts that we discussed. With the concepts we've covered, you're probably starting to see how keeping your fleet of machines, whether they're virtual or physical, off of a pedestal is good practice. If something breaks, goes down or catches fire literally or figuratively, you can easily spin up a replacement because you know exactly what it's supposed to look like from the configuration and you can deploy it easily using the automation. In the next module, we'll check out how you can deploy Puppet in your infrastructure and look into some more advanced configuration management and change management techniques. Before that, you'll have the opportunity to try out fixing a system where the configuration management isn't doing what it's supposed to. You'll see what running the Puppet agent looks like in practice and find out what's wrong with the deployed rules, and then get the automation to behave as expected. Cool, right? Let's go for it.

## Deploying Puppet

### Applying Rules Locally

Up to now we've been getting to know Puppet syntax and understanding the different resources available. It's now time for the next step, trying out some Puppet rules on our local computer. In an earlier video, we called out that Puppet is usually deployed in a client-server architecture. But that's not the only way we can use Puppet. We can also use it as a stand-alone application run from the command line. This is common when testing new configurations. It can be the preferred configuration for complex setups where connecting to a master is no longer the best approach. When using a stand-alone Puppet, the same computer processes the facts, calculates the rules that need to be applied, and makes any necessary changes locally. So to get started with our Puppet deployment, let's first install Puppet and then we can start experimenting with running rules locally. In later videos, we'll check out how to create a client-server deployments. As we've called out, Puppet is available on a number of different platforms. We can either install it from the package management system available in the OS or download it from the official website. Both options work fine and the best one to choose will depend on our specific needs. For this exercise, we'll just go with the Puppet packages provided by the Ubuntu distribution. We'll do that by installing the Puppet master package using sudo apt install puppet-master.
We now have the package installed and can start trying out a few rules. We'll begin by creating the simplest possible Puppet file. We can make it more complex as we improve our deployments. For this example, we want to use Puppet to make sure that some useful tools for debugging problems are installed on each computer in our fleet. **To do this, we first have to create a file where we'll store the rules that we want to apply. In Puppet lingo, these files are called manifests and they must end with a.pp extension.** So we'll create a new file called tools.pp and in this file, we'll create a package resource. We'll start by managing the htop package which is a tool similar to top that can show us some extra information. We'll state that we want Puppet to ensure that we have this package present on our computer. Cool. That was simple. That's all we have to do. This resource will take care of installing the package for us. Let's save the file and try it out. But before actually applying the rules, we want to check that the command isn't present yet.
Htop isn't installed yet. Let's fix that by running our rules using **sudo puppet apply -v tools.pp**.
The -v flag tells Puppet that we want to get verbose output which will tell us what's going on while Puppet is applying the rules in the file that we pass to it. So here, Puppet first told us that it was loading the facts. Then, that it compiled a catalog. After that, it told us that it was applying the current configuration. Then, that it installed the package we requested. Finally, it let us know that it finished applying this catalog. You're probably wondering, **what's a catalog? We called out in an earlier video that after loading all facts for a computer, the server calculates which rules actually need to be applied.** For example, if a packet should only be installed when a certain condition is met, this condition is evaluated on the server side based on the gathered facts. The catalog is the list of rules that are generated for one specific computer once the server has evaluated all variables, conditionals, and functions. In this example, the catalog will be exactly the same as our code because the code didn't include any variables, functions, or conditionals. More complex sets of rules can lead to different catalogs depending on fact values. It's now time to check if our rules actually works. Let's try running the htop command again now that Puppet has installed it for us.
Yes, this time it worked. If our computer was misbehaving, we could now use this tool to get a better idea why. But fortunately, our computer's on its best behavior. So we'll exit now using q. Let's see what happens if we try to apply the Puppet rules again now that the package is installed.
Puppet's smart. It noticed that the package is already installed so it didn't try to install the package again. This means it applied the catalog much faster because nothing had to be changed. We've now seen how to write a Puppet resource in a manifest file and then use puppet apply to apply those rules to one computer. Up next, we'll check out how we can manage relationships between different Puppet resources and what that looks like when applied.

### Managing Resource Relationships
In our last video, we wrote a very simple manifest which we then applied locally. That was a great way to practice applying Puppet rules, but it was super-simple. Let's challenge ourselves with something a bit more tricky. The Puppet manifests that we use to manage computers in our fleet usually include a bunch of different resources that are related to each other. You're not going to configure a package that's not installed and you don't want to start a service until both the package and the configuration are in place. Puppets lets us control this with resource relationships. Let's check this out in an example. We have a file called ntp.pp, that has a bunch of resources related to the NUTS configuration like the one we've seen in an earlier video.
This time, on top of declaring the resources that we need to manage, we're also declaring a few relationships between them. We see that the configuration file requires the NTP package and the service requires the configuration file. This way, Puppet knows that before starting the service, the configuration file needs to be correctly set, and before sending the configuration file, the package needs to be installed. We're also declaring that the NTP service should be notified if the configuration file changes. That way, if we make additional changes to the contents of the configuration file in the future, the service will get reloaded with the new settings. If you look closely, you might notice that the resource types are written in lowercase, but relationships like require or notify use uppercase for the first letter of the resource. **This is part of Puppet syntax. We write resource types in lowercase when declaring them, but capitalize them when referring to them from another resource's attributes.** This sounds confusing right now, don't worry. It might take a while to wrap your head around it, but it will eventually click. Now, one last thing. At the bottom of the file, we have a call to include NTP. That's why we told Puppet that we want to apply the rules described in a class. For this example, we put the definition of the class and the call to include the class in the same file. Typically, the class is defined in one file and include it in another. We'll checkout examples for this in later videos. All right. Let's apply these rules locally.
Great. Our rules have run and in the verbose output, we can see that it did a bunch of things. First, it installed the package, then it checked that the configuration file needed to be updated and so it changed its contents. Finally, after changing the contents of the configuration, Puppet knew to restart the NTP service. We see here how our Puppet rules have translated into a few different actions. That's cool, but it's about to get even better. Let's make a change to the configuration file by editing the ntp.com file in this directory.
This is the configuration values by the NTP service. It's currently using a bunch of servers from ntp.org. But instead of those servers, we want to try out the NTP servers provided by Google. These are called time1.google.com, and then time2, time3, and time4.
Start transcript at 3 minutes 42 seconds
We've made the change, we'll save with :WQ and then rerun our Puppet rules with the new configuration file.
Awesome. Puppet updated the configuration file with the new contents and then refresh the service, so it loaded the config. Success. In this video, we've seen how we can apply a Puppet manifests that includes a class with a bunch of resources. We grouped all of the information related to the NTP service in a manifest specific to it, which is common practice when dealing with Puppet rules. We want to keep related operations together and separate things that are unrelated. Up next, we'll look into how we can do that using Puppet modules.

### Organizing Your Puppet Modules

In any configuration management deployment, there's usually a lot of different things to manage. We might want to install some packages, copy some configuration files, start some services, schedule some periodic tasks, make sure some users and groups are created and have access to specific devices, and maybe execute a few commands that aren't provided by existing resources. On top of that, there might be different configurations applied to the different computers in the fleet. For example, workstations and laptops might include resources that aren't used on servers. Each distinct type of server will need its own specific setup. There's a lot of different things to manage. We need to organize all these resources and information in a way that helps us maintain them long-term. This means grouping related resources, giving the groups good names, and making sure that the organization will make sense to new users. In puppet, we organize our manifests into modules. **A module is a collection of manifests and associated data.** We can put any resource we want into a module, but to keep our configuration management organized, we'll group things together under a sensible topic. For example, we could have a module for everything related to monitoring the computer's health, another one for setting up the network stack, and yet another one for configuring a web serving application. So the module ship the manifest in the associated data, but how is this organized? 
**All manifests gets stored in a directory called manifests. The rest of the data is stored in different directories depending on what it does.** 
**The files directory includes files that are copied into the client machines without any changes, like the ntp.conf file that we saw in our last video.** 
**The template's directory includes files that are preprocessed before they've been copied into the client machines.** These templates can include values that get replaced after calculating the manifests, or sections that are only present if certain conditions are valid. There's a bunch more directories that can be part of a module depending on what exactly the module does. But you don't need to worry about these when creating your first puppet module. You can start with the simple module that just has one manifest in the Manifest directory. This file should be called init.pp and it should define a class with the same name as the module that you're creating. Then any files that your rules use need to be stored in the files or templates directories depending on whether you copy them directly or need to preprocess them. For example, this is how the NTP class that we saw in our last video looks like when turned into a module.

There's an init.pp file, which contains the NTP classes that we saw before, and the ntp.conf file that gets deployed onto the machine is now stored in the files directory. Modules like these can look pretty much the same no matter who's using them. That's why over time, system administrators using puppet have shared the modules they've written, letting others use the same rules. By now, there's a large collection of prepackaged modules that are shipped and ready to use. If one of those modules does what we want, we can just install it on our Puppet server and use it in our deployments. Let's install the Apache module provided by Puppet Labs to check out how this works.

We've installed the module. Let's have a quick look at its contents. First, we'll change into the directory where the module files are stored and list its contents.
We see the files, manifests, and templates directories that we mentioned. On top of that, there's a lib directory that adds functions and fact to the ones already shipped by puppet. The metadata.json file includes some additional data about the module we just installed, like which versions of which operating systems it's compatible with. Let's peek into the manifest directory.

That's a lot of files, like how we split the different things that we want to manage into separate modules. We can also split each separate functionality that we want to configure into separate manifests. This helps us organize our code when we make changes to it, and to see how this directory also contains its own init.pp. As we called out, this manifest is special. It needs to always be present because it's the first one that's read by puppet when a module gets included. So how do we include a module like this one? It's pretty easy. Let's create a manifest file that includes the module we've just installed.
Here, we're telling Puppet to include the Apache module. The double colon before the module name, let's puppet know that this is a global module. Let's save this file now and apply it using Puppet apply like we did before.
Start transcript at 5 minutes 50 seconds
Our manifest was super-simple, it just include the Apache module. But by including the module, we got puppet to apply all the rules run by default in the module. We now have an Apache server configured and ready to run on this machine. We've just seen how we can organize our code in modules and how we can even use modules provided by other teams so we don't have to reinvent the wheel. Up next, there's a reading with pointers to more information, followed by a quick quiz. After that, meet me over in the next video, where we'll check out what we need to do to deploy our rules to more machines.

**A module is an easy way to organize our configuration management tools.**

### Links

Check out the following links for more information:

https://puppet.com/docs/puppet/latest/style_guide.html
https://puppet.com/docs/puppetserver/latest/install_from_packages.html

### Puppet Nodes

When managing fleets of computers, we usually want some rules to apply to every computer, and other rules to apply only to a subset of systems. Let's say you're managing all your servers with Puppet. You might want to install a basic set of tools on all of them, but only install the packages for serving web pages in your web servers. And only install the packages for sending and receiving email in your mail servers. There's a bunch of different ways that we can do this. In an earlier video, we saw how to conditionally apply some rules using facts from the machines. Another way to apply different rules to different systems is to use separate node definitions. In Puppet terminology, a node is any system where we can run a Puppet agent. It could be a physical workstation, a server, a virtual machine, or even a network router, as long as it has a Puppet agent and can apply the given rules. So we can set up Puppet to give some basic rules to all the nodes, but then apply some specific rules to the nodes that we want to be different. Let's check out an example of how this could look. When setting up Puppet, we usually have a default node definition that lists the classes that should be included for all the nodes. For example, it could look something like this.

Here, the default node is including two classes, the sudo class and the ntp class. For the ntp class, we're setting an additional servers parameter that lists the servers we can use to get the network time.

As you can see here, when defining a node, you can include a class by just using its name if there's no additional settings, or include the class and set additional parameters if necessary. All right, that's the default node, so it will apply to computers in the fleet by default. What if you want some settings to only apply to some specific nodes? You can do that by adding more node definitions that list the classes that you want them to include, like this. **We can see here that specific nodes in the fleet are identified by their FQDNs, or fully qualified domain names.** In this case, we have the node definition for a host called webserver.example.com. For this node, we're including the same sudo and ntp classes as before, and we're adding the apache class on top. We're listing the same classes because the classes included in the default node definition are only applied to the nodes that don't have an explicit entry. In other words, when a node requests which rules it should apply, Puppet will look at the node definitions, figure out which one matches the node's FQDN, and then give only those rules. To avoid repeating the inclusion of all the common classes, we might define a base class that does the work of including all the classes that are common to all node types. Now, where's this information stored? The node definitions are typically stored in a file called site.pp, which isn't part of any module. Instead, it just defines what classes will be included for what nodes. This is another step towards helping us organize our code in a way that makes it easier to maintain. Up next, we'll look into the infrastructure used by Puppet to verify if a node really has the name that it claims to have.

**Different kinds of nodes are defined, allowing different sets of rule catalogs to apply to different types of machines.**

### Puppet's Certificate Infrastructure

We've called that a few times that in typical Puppet deployments, all managed machines and the fleet connect to a Puppet server. **The client send their facts to the server, and the server then processes the manifests, generates the corresponding catalog, and sends it back to the clients who apply it locally.** In our last video, we mentioned that we can apply different rules to different nodes depending on their names. The client send their name to the server when they connect, but how can the server trust that a client is really who he claims to be? It's a dangerous world out there. Well, this is a complex subject that touches on some important security concepts. We'll do a quick rundown here. If you're interested in learning more, you might want to check out the security course in the Google IT support professional certificate program led by my colleague, Gian, who explains it in more detail. **Puppet uses public key infrastructure, or PKI, to establish secure connections between the server and the clients.** There's a bunch of different types of public key technologies. The one used by Puppet is secure sockets layer or SSL. This is the same technology used for encrypting transmissions over HTTPS. The clients use this infrastructure to check the server's identity, and the server uses it to check the client's identity, and all communication is done over an encrypted channel that uses these identities so it can't be intercepted by other parties. So how does this work? Each machine involved has a pair of keys related to each other, a private key and a public key. The private key is secret, only known to that specific machine, the public key is shared with other machines involved. Machines can then use the standardized process to validate the identity of any other machine. The sender signs a message using the private key and the receiver validates the signature using the corresponding public key. Okay. But how do machines know which public keys to trust? This is where a certificate authority, or CA comes in. The CA verifies the identity of the machine and then creates a certificate stating that the public key goes with that machine. After that, other machines can rely on that certificate to know that they can trust the public key, since it means the machine's identity has been verified. Puppet comes with its own certificate authority, which can be used to create certificates for each clients. So you can use that one, or if your company already has a CA that validates the identity of the machines in your fleet, you can integrate it with Puppet, so you only validate the identities once. Now, let's assume you're using the baked-in certificate infrastructure and dive into how this process works. When a node checks into the Puppet master for the first time, it requests the certificate. The Puppet master looks at this request and if it can verify the nodes identity, it creates a certificate for that node. The system administrator can check the identity manually or use a process that does this automatically using additional information about the machines to verify their identity. When the agent node picks up this certificate, it knows it can trust the Puppet master, and the node can use the certificate from then on to identify itself when requesting a catalog. You might be wondering, why do we care so much about the identity of the nodes? There's a bunch of reasons. First, Puppet rules can sometimes include confidential information that you don't want to fall in the wrong hands. But even if none of the rules hold confidential info, you want to be sure that the machine you're setting up as your web server really is your web server and not a rogue machine that just claims to have the same name. All sorts of things could go wrong if random computers start popping up in your network with the wrong settings. If you're creating a test deployment to try out how Puppet rules get applied, and so you're only managing tests machines, you can configure Puppet to automatically sign all requests, but you should never do this for real computers being used by real users. Remember that it's better to be safe than sorry. So always take the time to authenticate your machines. When starting out with Puppet, it's common to use the manual signing approach. In this case, when the node connects to the master, it will generate a certificate request, which we'll go into a queue in the Puppet master machine. You'll then need to verify that the machine's identity is correct and the baked-in CA will issue the corresponding certificate. If your fleet is large, this manual approach won't really work. Instead, you'll want to write a script that verifies the identity of the machines automatically for you. One way to do this is by copying a unique piece of information into the machines when they get provisioned and then use this pre-shared data as part of the certificate request. That way, your script can verify that the machines are who they claim to be without involving any humans. Great, you now have a broad idea of the infrastructure that Puppet uses to identify the nodes when they connect to the master. Up next, we'll see what the typical Puppet setup using a separate Puppet server and client looks like in practice.

### Setting up Puppet Clients and Servers

sudo puppet config --section master set autosign true
ssh webserver

sudo puppet config set server ubuntu.example.com

sudo puppet agent -v --test

sudo systemctl enable puppet

sudo systemctl start puppet 

sudo systemctl status puppet

We're now ready to see a Puppet deployment in action. We've already installed the Puppet master package on this computer, so we'll use it as the master. Since this is a test deployment to demonstrate Puppet, we'll configure it to automatically sign the certificate requests of the nodes we add. But remember, if we were deploying this to real computers, we'd have to manually sign the requests or implement a proper validating script. We'll do this by calling the Puppet command with the config parameter, and then saying that in this section master we want to set auto sign to true. All right. With that, we can connect to the client that we want to manage using Puppet. We'll connect using SSH to a machine called web server. On this machine, we'll install the Puppet client which is shipped by the Puppet package.

Nice. We have the Puppet agent installed. Now we need to configure it to talk to the Puppet server that we're running on the other machine. To do that, we'll use Puppet config like before but this time we'll tell it that we want to set the server to ubuntu.example.com.

Great. Now that we've configured the server, we can test the connection to the Puppet master by using the Puppet agent command passing dash v as before to get verbose output, and dash dash test to do a test run. As usual, Puppet tells us everything it did. It first created an SSL key for the machine. It then read a bunch of information from the machine and used this to create a certificate request. The agent shows us the fingerprint of the certificate requested. If we were using manual signing, we could use this fingerprint to verify that the request and the server matches the one generated on the machine. The certificate was then generated on our puppet master. We don't see any entries for that because it happened on the other computer. But we see that this computer received a certificate and stored it locally. Once the certificate exchange completed, the agent retrieved all the information from the machine and sent it to the master. In exchange, it got back a catalog and applied it. The catalog applied almost immediately because we haven't actually configured any rules to be applied to our clients. We should go ahead and do that now. We'll go back to our Puppet master and create a couple of node definitions. **As we called out, node definitions are stored in a manifest file called site.pp** which is stored at the root of the nodes environment. We'll talk more about environments in a later video. For now, we just need to know that our client is trying to access the production environment. So the file that we need to create will be located in slash etc puppet code environments production manifests, and it will be called site.pp.

In this file, we'll create a couple of node definitions. We want to install Apache in our web server, so we'll create a node definition for the web server with the Apache class and node parameters for now, and we'll also add a default node definition. We'll keep it empty for now. We can add more classes in the future. All right. With that, we have our very basic node definition. We can now save this and run the Puppet agent on our web server machine again.

This time, the Puppet agent connected to the Puppet master and got a catalog that told it to install and configure the Apache package. This included setting up a bunch of different services. Up to now, we've been doing manual runs of the Puppet agent for testing purposes. Now that we know it's working fine, we want to keep Puppet running automatically. That way, if we make changes to the configuration, clients will automatically apply those changes without us having to do any manual steps. So to do that, we'll use the system CTL command, which lets us control the services that are enabled when the machine starts and those that are currently running. So we'll first tell the system CTL to enable the puppet service so that the agent gets started whenever the machine reboots, and then we'll tell system CTL to start the puppet service so that it starts running. Last step, we'll ask systems CTL for the status of the Puppet service to check that it's actually running. Awesome. That worked. The Puppet agent will keep regularly checking in with the master and ask if there are any changes that need to be applied to the machine. With that, you've seen Puppet in action using the server client model. We use the configuration we set in the Puppet master to manage the installation and configuration of software in our web server, and we set up the Puppet agent in the web server to keep running so that the configuration stays up to date. We've only seen the very basics of how to configure Puppet, but this can already give you an idea of how powerful configuration management can be. Pretty exciting, right? Up next, we've gathered more info on how to do the client-server set-up and after that, a quick quiz to check that everything is still making sense.

**The Certificate Authority creates an SSL key for the agent machine and creates a certificate request.**

### Links

Check out the following link for more information:

http://www.masterzen.fr/2010/11/14/puppet-ssl-explained/


## Automation in the Cloud

### Cloud Services Overview

**So when we say that a service is running in the Cloud, what do we actually mean?** It has nothing to do with those white fluffy things in the sky, **it simply means that the service is running somewhere else either in a data center or in other remote servers that we can reach over the Internet.** These data centers house a large variety of machines, different types of machines are used for different services. For example, some machines may have local solid-state drive or SSD, for increased performance while others may rely on virtual drives mounted over the network to lower costs. Cloud providers typically offer a bunch of different service types, the ones used most by users are in the Software as a Service category. **Software as a Service or SaaS, is when a Cloud provider delivers an entire application or program to the customer.** If you choose a Cloud e-mail solution like Gmail, a Cloud storage solution like Dropbox, or a Cloud productivity suite like Microsoft Office 365, there are only a small number of options for you to select or customize. The Cloud provider manages everything related to the service for you including deciding where it's hosted, ensuring the service has enough capacity to serve your needs, performing backups frequently and reliably, and a lot more. There's a lot of software being offered as a service by many different Cloud providers or other Internet companies. But of course, not all of our needs can be solved by prepackaged software, sometimes we need to develop our own. For some of the components of our software, we might choose to use Platform as a Service. **Platform as a Service or PaaS, is when a Cloud provider offers a preconfigured platform to the customer.** When we say platform here, it can be a bit confusing because there are lots of different platforms that exist under a PaaS model. Let's check out an example to understand this better. Say you need an SQL database to store some of your applications data, you could choose to host the database in your own hardware. To do this, you'd need to install an operating system on that computer and then install the SQL software on top of the chosen OS. This requires a basic understanding of all of these different pieces just to get the database running. There's a bunch of things that could go wrong and even if you can eventually solve all of them, it can take awhile. Instead, you could decide to use a Cloud provider that offers an SQL database as a service, that way you can just focus on writing SQL queries and using the platform, and let the Cloud provider take care of the rest. There's a bunch of different platforms offered as a service by Cloud providers, but of course they are unlikely to cover all of your needs. If you need a high level of control over the software you're running and how it interacts with other pieces in your system, you might want to choose Infrastructure as a Service. **Infrastructure as a Service or IaaS, is when a Cloud provider supplies only the bare-bones computing experience.** Generally, this means a virtual machine environment and any networking components needed to connect virtual machines, the Cloud provider won't care what you're using the VMs for. You could use them to host a web server, a mail server, your own SQL database with your own configuration settings, or a whole lot more possibilities. Running your IT infrastructure on the Cloud provider's IaaS offering is a very popular choice. There's a lot of different providers out there, big and small that offer a service where you can run virtual machines in their Cloud. **Some IaaS products include: Amazon's EC2, Google Compute Engine, and Microsoft Azure Compute.** Now no matter the service model and the provider you use, when you set up Cloud resources you'll need to consider regions. **A region is a geographical location containing a number of data centers, regions contain zones and zones can contain one or more physical data centers.** If one of them fails for some reason, the others are still available and services can be migrated without notably affecting users. Large Cloud providers usually offer their services in lots of different regions around the world. Generally, **the region and zone you select should be closest to your users, the further your users are from the physical data center the more latency they may experience.** This might sound a bit strange but imagine if you are on vacation overseas, you might notice that your bank website loads a little slower. That's why it's common practice to locate data centers close to where users actually live, work, and bank. Latency isn't the only factor to take into account when selecting a region or zone, **some organizations require their data to be stored in specific cities or countries for legal or policy reasons.** **If your service uses other services as dependencies, it's a good idea to host the service physically close to its dependencies.** For example, if a mail server requires a database server to send an e-mail, it makes sense to host the database server and the mail server in the same zone. Recall that earlier, that Qwiklabs is a service using Cloud infrastructure. So what kind of Cloud service does Qwiklabs use? Qwiklabs uses Infrastructure as a Service, the VMs get provisioned with just the OS and the lab automation then deploys any additional files and software into the OS. Up next, we'll talk about how we can use the services offered by Cloud providers to help us scale our applications.

**Infrastructure as a Service (IaaS) provides users with the bare minimum needed to utilize a servers computational resources, such as a virtual machine. It is the user's responsibility to configure everything else.**

### Scaling in the Cloud

One of the coolest features of deploying solutions to the Cloud is how easily and quickly we can scale our deployments. In a traditional IT setting, if your team needs an extra server to improve the service, you need to buy additional hardware, install the operating system and application software and then integrate the new computer with the rest of the infrastructure. Doing all of these takes time so it's not easy to quickly scale up or down if the service gets more or less usage. **In other words, it takes a significant amount of time to modify the capacity of the deployment. In this context, capacity is how much the service can deliver.** The available capacity is tied to the number and size of servers involved. We get more capacity by adding more servers or replacing them with bigger servers. The way we measure the capacity of a system depends on what the system is doing. If we're storing data, we might care about the total disk space available. If we have a web server responding to queries from external users, we might care about **the number of queries that can be answered in a second which is called queries per second or QPS.** Or maybe the total bandwidth served in an hour. We can measure capacity in other fun ways like the number of cat videos served in an hour or the number of digits of pi a system can calculates. Our capacity needs can change over time. Say you're hosting an e-commerce site that needs a hundred servers to meet user demands. As the service becomes more popular, demand might grow and you'll need to increase the available capacity. Eventually, the system could need a thousand servers to meet user demands. **This capacity change is called scaling. In particular, we call it upscaling when we increase our capacity and downscaling when we decrease it.** This could happen for example if the demand for a product decreased or if the system was improved to need fewer resources. Cloud providers typically have a lot of available capacity that can be used by their customers. When we choose to host our infrastructure in the Cloud, we're purchasing and using some of the providers capacity to supplement or completely replace our on-premise capacity. This lets us easily scale our service to satisfy demand. There are a couple of different ways that **we can scale our service in the Cloud, horizontally and vertically.** **To scale a deployment horizontally, we add more nodes into the pool that's part of a specific service.** Say your web service is using Apache to serve web pages. By default, Apache is configured to support a 150 concurrent connections. If you want to be able to serve 1,500 connections at the same time, you can deploy 10 Apache web servers and distribute the load across them. This is called horizontal scaling. You add more servers to increase your capacity. If the traffic goes up you could just add more servers to keep up with it. On the flip side, **if you're scaling a deployment vertically, it means you're making your nodes bigger. When we say bigger here we're talking about the resources assigned to the nodes like memories, CPU, and disk space.** For example, a database server with a 100 gigabytes of disk space can store more data than with only 10 gigabytes of space. To scale this deployment we can just add a bigger disk to the machine and the same idea works for a CPU and memory too. Say you have a caching server and you notice it's using 95 percent of the available memory. You can deal with that by adding more memory to the node. Depending on our deployment and our needs, we might need to scale both horizontally and vertically to scale the capacity of our service. In other words, adding more and bigger nodes to our pool. This approach to scaling isn't too different from what you'd need to do if you have your servers running on-premise. Instead of sending someone to change the physical deployment, for example adding more physical RAM to a server or adding 10 more physical machines in a server rack, we just modify our deployment by clicking some buttons in a web UI or using a configuration management system to automate the scaling for us. The infrastructure built by the Cloud provider will deploy any additional resources we need. When talking about scaling in the Cloud, another aspect we need to take into account is whether the scaling is done automatically or manually. When we set our service to use **automatic scaling, we're using a service offered by the Cloud provider. This service uses metrics to automatically increase or decrease the capacity of the system.** Say you have a system that currently has the capacity to serve 1,000 cat videos per minutes. If the demand for these videos increases to 10,000 per minute and it will, the software in-charge of the automatic scaling will add resources and increase the overall capacity to meet this demand. When the users stop watching cat videos, the automation will remove any unused resources, so the operating costs stay small. But really who wants to stop watching cat videos? But make sure you **set a reasonable quotas for your autoscaling systems**. Otherwise, that viral video of a cute cat wearing a hat might surprise you with a very uncute big bill from your Cloud provider. On the flip side, **using manual scaling means that changes are controlled by humans instead of software**. Manual scaling has its pros and cons too. When the Cloud deployment isn't very complex, it's usually easier for smaller organizations to use manual scaling practices. Say your company currently has a single mail server and you know that you'll want to have another one in six months. In that case, there's no need to overcomplicate that system with an autoscaler. You could simply add the extra server sometime along the way. The trade-off here is that without good monitoring or alerting, a system without autoscaling technologies might suffer from unexpected increases in demand. If you're using manual scaling for a service that becomes popular and demand grows quickly, you might not be able to increase the capacity quickly enough. This can store up lots of problems ranging from poor performance to an actual outage. In this video, we've covered concepts that are central to any solution hosted in the Cloud like capacity and scaling. As you probably noticed, Cloud technology offers a ton of benefits for an IT team. But it also can be a little intimidating. Up next, we'll go into some of the reasons why IT teams might be hesitant to migrate to the Cloud and how to overcome those fears.

### Evaluating the Cloud

If you've always worked in a traditional IT environment with servers that are physically owned by your company, the idea of migrating to the cloud can be pretty scary. When you're running the service yourself, if something breaks, you can either physically walk up to the server to fix it or SSH into it from inside the same network. You can apply a quick fix and have your users back to being productive in no time.

As part of the IT team, you own the hardware, software, the network connections, and anything in between, which lets you have a lot of control over what's going on in the whole system. In the case of cloud solutions, we need to give up some of this control to the cloud provider.

We have different levels of control depending on the service model that we choose, whether that's software, platform, or infrastructure as a service. **When choosing to use software as a service, we're basically giving the provider complete control of how the application runs.** We have a limited amount of settings that we can change, but we don't need to worry about making the system work. This can be a great option when the software provided fulfills all of our needs and we'd rather just focus on using the software instead. But as we called out, there's only a limited amount of applications being offered in such a prepackaged way. **If we need to create our own applications, we can use platform as a service.** With this option, we're in charge of the code, but we aren't in control of running the application. Or we can choose infrastructure as a service, where we can still keep a high level of control. We decide the operating system that runs on the virtual machines, the applications that are installed on it, and so on. We'll still depend on the vendor for other aspects of the deployment, like the network configuration or the services availability. If something does break, you might need to get support from the vendor to fix the problem. So when choosing a cloud provider, it's important to know what kind of support is available and select the one that fits your needs. I know it sounds strange to give away your control over the hardware, and the network, and the overall infrastructure. But personally, I find that it's pretty great to not have to worry about maintaining the machines that are running our services. It means we can treat the servers executing the workloads as a commodity, instead of special snowflakes. One aspect that might make you hesitant to move to the cloud is that you don't know exactly what security measures are being put in place. So **when selecting which provider to use, it's important that you check how they're keeping your instances and your data secure.** There are a bunch of certifications like SOC 1, ISO 27001, and other industry recognized credentials that you can look for to verify that your provider has invested in security. Once you're sure that your provider is taking the right security measures, it might be tempting to just leave security to the professionals and forget about it. But as cloud users, we also have a responsibility to follow reasonable security practices. **Google, Amazon, Microsoft, and other cloud providers invest heavily in security research.** But that won't matter if the root password of your cloud instance password one or the instance doesn't use a firewall. In other words, we should always use reasonable judgment to protect the machines that we deploy ,whether that's on physical server is running on-premise or on virtual machines in the Cloud. It's also important to keep in mind that security systems can be expensive to implement correctly. Some highly sensitive deployments might warrant specialized security procedures, like multi-factor authentication, encrypted file systems, or public key cryptography. But these processes can also be expensive to implement. It's worth considering if using these techniques is necessary for your specific use case. If your application stores recent patient health records, that's super important data that needs to be protected. You want to apply the most stringent security practices. But if you're dealing with patient health records from the 1800s, you'll need less comprehensive security measures, since this data is much less sensitive, given its age. There's a bunch of other reasons why you might have doubts about cloud providers. For example, you might be worried of where your data is going to be stored. Or you might fear that the support offered won't satisfy your needs. No matter the reason, It's important that you carefully read the terms of service to understand the conditions and figure out if the service offered will satisfy your needs. In a way, cloud services are a little like actual clouds. They come in all different shapes and sizes. And sometimes a dark stormy one comes along to rain on your productive day.

But if you prepare an advance with the right security measures and maybe an umbrella, working in the cloud will be nothing but a breeze. So let's say you've decided to migrate part of your infrastructure to the cloud. What do you do next?

Migrating to the cloud is a big topic, and it's coming up in our next video.

**Cloud services provide many advantages, including simplifying configuration management, outsourcing support and maintenance, and letting the provider take care of security.**

**Cloud services provide several advantages, like putting the provider in charge of security.**

### Migrating to the Cloud

A lot of companies today are looking into migrating at least part of their IT infrastructure to the Cloud. The details of the migration will depend on what your infrastructure currently looks like, and what you're trying to achieve by migrating to a Cloud provider. In general, **we're looking at a trade-off between how much control we have over the computers providing the services and how much work we need to do to maintain them**. We've called out that when we use Infrastructure as a Service or IaaS, we deploy our services using virtual machines running on the Cloud providers infrastructure. We have a lot of control over how the infrastructure is designed which can be super useful. For example, we can decide which of the many available machine types to use and what kind of storage to attach to them. **IaaS is especially useful to administrators using a lift(changing the location of your physical machine) and shift(deploying all the contents of your physical machine on a cloud) strategy.** So what does that mean? Say you work at a small organization that's expanding. As the company grows, physical space for employees; desks, ping pong tables, and printers becomes scarce. Eventually, the whole office might need to move to a larger space. This means moving not just the desks and printers, but also any servers running on-premise. If physical servers need to be moved, you might need to take a server from the old office, turn it off during a maintenance window, load it onto a truck, and physically drive it to the new location. This could be the new office or maybe even a small data center. So you're literally lifting the server and moving it to a new location, that's where the lift in lift and shift comes from. When migrating to the Cloud, the process is somewhat similar. But instead of moving the physical server in the back of a truck, you migrate your physical servers running on-premise to a virtual machine running in the Cloud. In this case, you're shifting from one way of running your servers to another. The key thing to note with both approaches, is that the servers core configurations stay the same. It's the same software that needs to be installed on the machine to provide its functionality, no matter if the server is hosted physically on-site or virtually in the Cloud. If you've already been using configuration management to deploy and configure your physical servers, moving to a Cloud setup can be pretty easy. You just have to apply the same configuration to the VMs that are running in the Cloud and you'll have replicated the setup. On the flip side, using this strategy means that you still have to install and configure the applications yourself. You need to make sure that both the OS and the software stay up to date, that no functionality breaks when they get updated, and a bunch of other things depending on which specific application the server is running. One alternative in this case is using Platform as a Service or PaaS. This is well-suited for when you have a specific infrastructure requirement, but you don't want to be involved in the day-to-day management of the platform. In an earlier video, we mentioned the example of an SQL database that could be used in this way. By leaving the management of the database to the Cloud provider, you don't need to worry about having the right disks attached to the computer, configuring the database or any other task related to the machine setup. Instead, you can focus on just using the database. **Another example of Platform as a Service are managed web applications.** When using this service, you only have to care about writing the code for the web app. You don't need to care about the framework for running it. This can accelerate development because developers don't have to spend time managing the platform and can just focus on writing code. **Some popular managed web application platforms include Amazon Elastic Beanstalk, Microsoft App Service, and Google App Engine.** While these platforms are very similar, they aren't fully compatible. So migrating from an on-premise framework and switching between vendors will require some code changes. Another related concept that you might have heard of is containers. **Containers are applications that are packaged together with their configuration and dependencies.** This allows the applications to run in the same way no matter the environment used to run them. In other words, if you have a container running an application, you can deploy it to your on-premise server, to a Cloud provider, or a different Cloud provider. Whichever you choose, it will always run in the same way. This makes migrating from one platform to the other super easy. **When talking about migrating to the Cloud, you may also hear about public Clouds, private Clouds, hybrid Clouds, and multi-Clouds.** Let's check out what each of these mean. We call **public Cloud** the Cloud services provided to you by a third party. **It's called public because Cloud providers offer services to, you guessed it, the public.** 
**A private Cloud is when your company owns the services and the rest of your infrastructure, whether that's on-site or in a remote data center.** It's private because it's just for your company, like having your own Cloud in the sky. 
**A hybrid Cloud is a mixture of both public and private Clouds.** In this scenario, some workloads are run on servers owned by your company, while others are run on servers owned by a third party. The trick to making the most of the hybrid Cloud is ensuring that everything is integrated smoothly. This way, you can access, migrate, and manage data seamlessly no matter where it's hosted. Finally, **multi-Cloud is a mixture of public and/or private Clouds across vendors.** For example, a multi-Cloud deployment may include servers hosted with Google, Amazon, Microsoft, and on-premise. A hybrid Cloud is simply a type of multi-Cloud, but the key difference is that multi-Clouds will use several vendors, sometimes in addition to on-site services. Using multi-Clouds can be expensive, but it gives you extra protection. If one of your providers has a problem, your service can keep running on the infrastructure provided by a different provider. In the last few videos, we've covered a lot of different concepts related to Cloud infrastructure, Cloud services, and how we can use them to scale in the Cloud. Coming up, we've got a quiz to check that all of these concepts are making sense. After that, we'll look into how using Cloud infrastructure looks in practice.

### Spinning up VMs in the Cloud

We've been talking a lot about how the Cloud works, what the different concepts that play are, and what they mean. In the next few videos, we'll be showing you how some common actions that you might perform on the Cloud look in practice. As we've called out, there's a bunch of different Cloud providers that you can use for your projects, each with some specific advantages depending on what you're trying to achieve. And while some terms used by one provider might not exactly match the ones used by other providers, the concepts are the same. In these videos, we'll use the Google Cloud platform to demonstrate our examples because, well, it's the platform we know best. All Cloud providers give you a console that lets you manage the services that you're using. This console includes pointers to a lot of different services that the providers offer. Seeing all of the options available, it can be a little dizzying at first. So it's a good idea to start just by familiarizing yourself with the platform before you try to do something with it. This can mean, for example, looking at the available menus and options, and figuring out where the sections that let you use infrastructure-as-a-service are located. No matter the exact menu entries, when you want **to create a VM running in the Cloud, there are a bunch of parameters that you need to set**. These parameters are used by the Cloud infrastructure to spin up the machine with the settings that we want. **You'll start by choosing the name assigned to the instance.** This name will later let you identify the instance if you want to connect to it, modify it, or even delete it.

**You'll also have to choose the region and zone where the instance is running.** As we called out in an earlier video, you'll generally want to choose a region that's close to your users so that you provide better performance. **Another important option that you'll need to select is the machine type for your VM.** Cloud providers allow users to configure the characteristics of their virtual machines to fit their needs. This means selecting how many processing units, or virtual CPUs, and how much memory the virtual machine will be allocated. You might be tempted to select the most powerful VM available, but of course the more powerful the VM, the more money it will cost to run it. As a sysadmin, you may need to decide between costs and processing power to fit the needs of your organization. When setting up instances like these, it's a good idea to start small and scale as needed. On top of the CPU and memory available, **you'll also need to select the boot disk that the VM will use**. Each virtual machine running in the Cloud has an associated disk that contains the operating system it runs and some extra disk space. **When you create the VM, you select both how much space you want to allocate for the virtual disk and what operating system you want the machine to run.** To create these resources, we can use the web interface or the command line interface. The web UI can be very useful for quickly inspecting the parameters that we need to set. **The UI will let us compare the different options available and even show us an estimation of how much money our selected VM would cost per month.** This is great for experimenting, but it doesn't scale well if we need to quickly create a bunch of machines or if we want to automate the creation. In those cases, we'll use the command line interface, which lets us specify what we want once, and then use the same parameters many times. **Using the command line interface lets us create, modify, and even delete virtual machines from our scripts.** This is a great step towards automation, but it doesn't stop there. We can also automate the preparation of the contents of those virtual machines. Imagine spending an afternoon installing and configuring your new web server. You can do this on one machine, and the process is fairly straightforward. You install any necessary software, you modify any configuration settings, and then make sure that it's working correctly. But it would be hard to reproduce this exactly on another machine, and impossible to do it on thousands of machines. This is where reference images and templating come into play. **Reference images store the contents of a machine in a reusable format, while templating is the process of capturing all of the system configuration to let us create VMS in a repeatable way.** That exact format of the reference image will depend on the vendor. But often, the result is a file called a disk image. **A disk image is a snapshot of a virtual machine's disk at a given point in time.** Good templating software lets you copy an entire virtual machine and use that copy to generate new ones. Depending on the software, the disk image might not be an exact copy of the original machine because some machine data changes, like the hostname and IP address. But it will have the data that we need to make it reusable on lots of virtual machines. This can be super helpful if we want to build a cluster of 10,000 machines which all have identical software. Up next, we'll do a few demos of this process. We'll show you how we can create new VMs in Google Cloud Console, how we can customize those VMs, and how we can use templating and reference images to automate the creation.

### Creating a New VM Using the GCP Web UI

Okay, let's get started with creating a virtual machine on our GCP project. We'll kick things off by **navigating to console.cloud.google.com**, which is where you'll find the cloud console for GCP. Here, the first step is to **create a project so that our VMs are associated to that project**. **We need to give our project a name**, let's name it First Cloud Steps.
Our project is being created, it takes a couple of seconds.
Now that we have a project, our dashboard has a lot more info. Next, we want to go to the menu entry that lets us create virtual machines. To do that, we'll **go into the Compute Engine menu**, and **select the VM instances entry**.
This screen is pretty empty because we don't have any VMs yet. We can **create a VM by pressing the Create button**.
Here we're showing the many different options that **we can set for this VM that we're creating. We can set the name, the region and zone, the machine type, the boot disk, and so on**. We'll start by calling this machine linux-instance.
Now it's time to select the region and zone. If we click on the region drop-down, we can see all the regions that are currently available to create new VMs. If we click on the zone drop-down we can see the zones available in that region for new VMs. For this example, we'll just keep the default regions. But as we called out, if you're deploying a service, you should select something that's close to your users. Next, we need to select the type of machine that we want to use. We can select between general purpose and memory optimized. And among each of those families, we can select a bunch of different machine types. We can select how much CPU and how much memory we want our VM to have.
The right selection will depend on what we plan to do with the computer. For our example, we'll just keep the default machine. After selecting the VM, we need to select the disk that we want to use. The default disk is 10 gigabytes in size and comes with a Debian image on it. We can select a different size or different OS by clicking on the Change button.
There's a long list of available operating systems to choose from. The right option will depend on what you're trying to do with your instance. For this example, we'll choose one of the Ubuntu versions. We can select which type of disk we want to use, either the standard disk which is cheaper, or the SSD version which is faster. And we could also change the size if we needed extra storage for our server. For now, we'll just keep the default values here.
After the boot disk, we're shown options to determine how access to the machine will work. This can be very simple or very complex, depending on the rest of your project. The default access option allows you to access the instance remotely using SSH, so we'll go with that one for now.
And finally, the creation wizard lets us pre-configure some firewall rules. Selecting one of these two options would let HTTP or HTTPS traffic reach our machine.
Of course, there are more firewall rules that you might want to set. Those can be set later on, after the machine is created. In a later video, we'll want to connect to a web server on this machine, so let's turn HTTP on.
There are a lot more options we can set, which are tucked away under this link. We won't look into those now, since the defaults make sense for our test machine. But you can check them out on your own to see what other parameters you can set. We're basically ready to create our VM, but before we do that, let's click on the command line link. This will show us how we would create the same VM through the command line.
Start transcript at 4 minutes 18 seconds
Wow, that's a long command line, but don't worry, you don't need to understand all of those parameters. The takeaway here is that you could select all the options that you want to create the VM that you need, and then copy this command to create a bunch of VMs that are all exactly the same as the one you selected. For now, we'll close this window and then create the VM using the Create button.
Our instance is being created, this takes a bit of time. The system is assigning the necessary resources to our machine, deploying the operating system image, connecting the network interfaces, and so on. Once it's done setting up, we can connect to it using SSH.
Again, it takes a little while for the system to set up the keys that we'll use to log on. But once it's done, we can use the machine remotely.
Let's check that the machine we created is using the OS we selected.
And with that, we've just created a VM using the web interface and connected to it using SSH, how cool is that? Once you're logged into the machine, you can treat it like any normal Linux machine, which is pretty awesome. For example, **we can get a text version of the weather in our current location by calling the curl command, which we can use to access web pages from the command line, and passing in wttr.in as the website.**

### Customizing VMs in GCP

In our last video, we checked out how to create a single virtual machine in the cloud. That's cool, but not too useful at cloud scale. Remember, cloud scale deployments are often comprised of hundreds or thousands of machines. So creating a single server is only the beginning. Let's make some changes to that VM so that we can deploy it at scale. Once we're done, we'll use the instance that we configured as the base for our reference image. Remember that **a reference image is just a file or configuration that we can deploy repeatedly and with automated tools**. This is important because it lets us build scalable services very quickly. Let's start by logging into the virtual machine we created in the last video.
We'll use git which will let us clone the repository with the code for the app we want to deploy.
The repo we've cloned includes a very simple web serving application written in Python. Let's run it to see what happens.
**Our script prints a single line saying that it's listening for connections on port 8000. What's happening behind the scenes is that the application is opening a socket and listening for HTTP connections on that port.** In this case, it's running on port 8000. And if we were running this locally on our machine, we could connect to that port. But this is running on a virtual machine in the cloud which has a firewall and only a couple of ports enabled. What are our options? The script actually lets us pass the port number that it will open as a parameter.
We want it to run on the HTTP port that we configured in our last video which is port 80. And because this is a system port, to let our application use it, we'll need to run it with admin privileges. So let's stop the running process now by pressing Ctrl+C. And then run it again with sudo and pass port 80 as the parameter.
**sudo ./hello_cloud.py 80**
Stop sudo 80.
Now we can visit the website served by our VM and see its contents. Let's navigate to it.
Our web app is extra symbol. It just prints Hello Cloud to the web page generated when we make a request. It also prints the Hostname and IP Address of the machine. This will help us later on when we deploy the solution at scale. All right, we have a web serving application running on the HTTP port. That's nice, but we had to start the application manually so this doesn't scale. **To get our application to start automatically, we need to configure this as a service.** Fortunately, our repo already includes a service definition that we can use. Let's check out the contents of that file.
**This is a systemd file, which is the initializing system used by most modern Linux distributions.**
This is a systemd file, which is the initializing system used by most modern Linux distributions.
You don't need to understand the details of this file to know how to deploy services to the cloud. Just notice that **the configuration expects the script that we want to execute to be in /usr/local/bin.** We need to **copy that file over to there (sudo cp hello_cloud.py /usr/local/bin/) and then copy the service file to /etc/systemd/system(sudo cp hello_cloud.py /etc/systemd/system/), which is the directory used for configuring systemd services**. And finally, we need to **tell the systemctl command that we want to enable this service so that it runs automatically(sudo systemctl enable hello_cloud)**. Okay, now that we've done this, anytime this machine starts, it will start the web app that we've configured and we'll be able to see the content that we saw before. Let's try it out by **triggering a reboot**(sudo reboot).
We've rebooted the machine. This will take a while to complete. It tells us that the connection was lost and that we can ask our terminal to reconnect. This will take a bit of time until the machine has finished rebooting and is ready to receive connections, patience, my friend.
Okay, our VM has rebooted. We can check if our application is running by using the ps ax command to get a list of the running processes and filter it so we keep only the ones matching a pattern using the grep command. In this case, we'll use hello as the pattern.
Yay, our application is now running on startup. We're almost ready to turn our configured VM into a template for creating a lot more of them. But before we do that, we need to **think about how we'll upgrade our web app when we want to make changes to it**. There's a bunch of different options here. **One option is to create a different reference image each time there's a new version of the app**. This would mean deleting all the old VMs and creating new ones based on the new image.
**Another option is to add a configuration management system to the images so that we can use that to manage any changes after the VM's created.** We already know how to manage changes with Puppet. Remember our Puppet Master training from earlier videos? Let's install the Puppet client in this instance so it's ready to use Puppet in the future.
Now when we looked into the Puppet Server and client setup, we saw that there was a bunch of steps that we need to run on the client side to have it ready to apply the rules. The repo we cloned includes a script we can run which will do the initial configuration for us. It will also set the Puppet process to run automatically on boot. Let's run that now.
Setup, Now any time this machine starts, it will serve our website and we want to update that website's content. We can do that using our Puppet infrastructure. Nice, our VM is now ready to be used as a basis for a template, which we can use to create as many instances as we need. Up next, we'll check out how to create the template and how to create instances based on it.

### Templating a Customized VM

In the last few videos, we created a VM, and then made sure that it was set up to serve our web app, and to stay updated via Puppet. We can now use this VM as a basis for creating an instance template, and then use the template to create a bunch of VMs based on it. Let's do that.
To create a reference image, we need to have access to the current virtual disk that's running on the computer. **So the first step to create the image is to stop the VM.** It takes a while for the machine to shut down cleanly. Once it's finished, we can **click on the machine's name** to see all its details, and then we can **click on the boot disk**.
These are the details of the disk attached to the VM. We can create a snapshot, which is a full copy of the current state of the disk, or an image, which lets us create a template based on it. Let's click create image.
We'll call our image webserver-image. Here, the creation wizard shows that we'll be creating the image based on the Linux-instance disk, which is what we want. For this example, we'll leave the rest of the settings with the default values. Okay, let's create our image.
This is now creating the image that we'll use for our template. As we called out earlier, the tools will keep most of the contents of the image, but remove things that should be different across VMs. Once it's finished creating, it shows us the list of all images that we can access. As you can see, this is a long list that includes a bunch of stuff along with our image. The other images are public images that we can use to deploy different types of VMs. All right. We're now ready **to create our instance template**. To do that, we'll **go to the instance template option**, and then **click create new instance template**. As usual, we're shown a wizard that includes a bunch of different options that we can set. We'll keep most of the defaults, and change only a couple of things. We'll name our template web server- template. We'll change the boot disk to use the image we've created.
In this screen, we can see the list of all the available images. By default, the list shows the official operating system images provided by the platform. **For our template, we want to use the custom Image we've created.**
And finally, **we'll also want to enable HTTP access to the instances created with this template**. That's it. We're ready to create our new template.
This takes a little bit of time to create. **Once it's done, we can create instances based on our images.** We'll do it once more from the web interface, and then we'll check out how to do it from the command line. Let's go back to the VM instances entry, and then click on create instance.
This time, instead of creating an instance from scratch, we'll use the template we've prepared. We'll name our instance web server- one. Everything else we'll leave as is. **Check out how it says that it will use the base image we selected, and that HTTP traffic will be allowed.**
All right, we've created our second VM based on the template. We didn't have to change any options, because all of the values were already pre-selected in the template. And the web app that we want is ready to run, without us having to configure anything. Let's check that out.
Yes, our application is already running successfully on this machine. This is great, but it's still a bit cumbersome if we want to create ten VMs like this one.
For a batch action like that, it's better to use the command line interface. So let's do that. **To interact with Google Cloud, we'll be using the gcloud command.** We've already installed the gcloud command on this machine. You'll find pointers on how to install gcloud on different platforms in the next reading. We'll start by running the **gcloud init** command, which **sets up the authentication mechanisms between this computer and Google Cloud**.
We need to authenticate to the Google Cloud system to be able to use the gcloud command to interact with it. Let's say yes here.
This opened a new tab in our browser that we can use to authenticate with our account. Let's follow the process here, and authenticate.
We're now logged into our cloud account. We can choose which will be our default project. Let's select one here.
We've selected the default project. On top of that, the initializing wizard lets us select the default region and zone. It's a good idea to select this, as the commands that we use in the future will use that zone and region if we don't specify a different one. This is a long list. There are a lot of different zones available for our instances. As we called out earlier, when selecting where to run your services, you should go with the one that's closest to you. For this example, we'll just go with Zone 1. Once we've completed this authentication, we can use the gcloud command to operate on our Cloud project. We can modify the VMs we've created, create new ones, delete some of the existing ones, and a lot more. For our example, we'll use it to create five additional VMs. It goes like this. First, we call gcloud, then we pass the compute parameter that's used for everything that has to do with virtual machines. Then we pass the instances parameter, as we'll be dealing with the VM instances themselves.
So then, we pass create, as we want to create instances. We'll see that we want to use the source instance template called web server- template. And finally, we'll give the name of the instances that we want to deploy. Let's call them ws1, ws2, ws3, ws4, and ws5.**gcloud compute instances create --source-instance-template webserver-template ws1 ws2 ws3 ws4 ws5** It takes only a short while until all instances are created. This is definitely much faster than going through the web interface, and much easier to automate through our scripts.
And with that, we've seen how we can create a virtual machine, customize it, create a template out of it, and use that template to create a bunch of new identical virtual machines. I hope you're starting to see how useful this can be when creating new IT deployments. Up next, there's a reading where you'll find a bunch more information about all the tools that we've demonstrated, and then a quick quiz to practice everything that you've just learned.

### Links

Over the last few videos we learned how to create and use virtual machines running on GCP. We then explored how we can use one VM as a template for creating many more VMs with the same setup. You can find a lot more information about this in the following tutorials:

https://cloud.google.com/compute/docs/quickstart-linux
https://cloud.google.com/compute/docs/instances/create-vm-from-instance-template
https://cloud.google.com/sdk/docs


### Cloud Scale Deployments

Over the past few videos, we've checked out some of the features we can use when running services in the Cloud. As we called out before, the biggest advantage of using Cloud services is how easily we can scale our services up and down. Now, to make the most out of this advantage, we need to do some preparation. **We'll set up our services so that we can easily increase their capacity by adding more nodes to the pool. These nodes could be virtual machines, containers, or even specific applications providing one service.** Whenever we have a service with a bunch of different instances serving the same purpose, we'll use a load balancer. **A load balancer ensures that each node receives a balanced number of requests.** When a request comes in, the load balancer picks a node to serve the response. **There's a bunch of different strategies load balancer uses to select the node.** **The simplest one is just to give each node one request called _round robin_.** **More complex strategies include always selecting the same node for requests coming from the same origin, selecting the node that's closest to the requester, and selecting the one with the least current load.** As we called out, instance groups like these are usually configured to spin up more nodes when there's more demand, and to shut some nodes down when the demand falls. This capability is called **autoscaling. It allows the service to increase or reduce capacity as needed while the service owner only pays for the cost of the machines that are in use at any given time.** Since some nodes will shut down when demand is lower, their **local disks will also disappear and should be considered ephemeral or short-lived.** If you need data persistence, you'll have to create separate storage resources to hold that data and connect that storage to the nodes. That's why the services that we run in the Cloud are usually connected to a database which is also running in the Cloud. This database will also be served by multiple nodes behind a load balancer, but this is typically managed by the Cloud provider using the platform as a service model. To check out how this works in practice, let's look at an example of a web application with a lot of users. **When you connect to a site through the Internet, your web browser first retrieves an IP address for the website** that you want to visit. **This IP address identifies a specific computer, the entry point for the sites.** Commonly there will be a bunch of different entry points for a single website. This allows the service to stay up even if one of them fails. On top of that, it's possible to select an entry point that's closer to the user to reduce latency. In a small-scale application, this entry point could be the web server that serves the pages, and that would be it. **For large applications** where speed and availability matter, **there will be a couple of layers in between the entry point and the actual web service**. **The first layer will be a pool of web caching servers with a load balancer to distribute the requests among them**. _One of the most popular applications for this caching is called Varnish, but of course it's not the only one. The Nginx web server and software also includes this caching functionality. There's a bunch of providers that do web caching as a service like Cloudflare and Fastly._ No matter the software used, the result is basically the same. When a request is made, the caching servers first check if the content is already stored in their memory. If it's there, they respond with the contents, if it's not, they ask their configured backend for the content and then store it so that it's present for future requests. **This configured backend is the actual web service that generates the webpages for the site, and it will also normally be a pool of nodes running under a load balancer.** To get any necessary data, this service will connect to a database. **But because getting data from a database can be slow, there's usually an extra layer of caching, specific for the database contents.** _The most popular applications for this level of caching are Memcached and Redis._ As you can see, there is a lot of different nodes in this scheme. Fortunately, once you've done your homework and prepared your setup, you can rely on the capabilities offered by the Cloud provider to automatically scale the system up and down as necessary. **The infrastructure will take care of adding and removing instances, distributing the load, making sure that each geographical region has the right capacity**, and a bunch more things. Up next, we'll talk about some of the things we can do when we need to have more control over the instances that are running our service.
**Load balancers reroute requests in order to balance and reduce network load.**

### What is orchestration?

Throughout this course and the entire program, we've been talking about automation. As a reminder, **automation is the process of replacing a manual step with one that happens automatically**. In the past few videos, we've mentioned a few ways that let us automate the creation of Cloud instances. **We can use templating to create new virtual machines, we can run a command line tool that automatically creates new instances for us, or we can choose to enable auto-scaling and let the infrastructure tools take care of that depending on the demand.** But all of this automatic creation of new instances needs to be coordinated so that the instances correctly interact with each other and that's where orchestration comes into play. **Orchestration is the automated configuration and coordination of complex IT systems and services.** In other words, orchestration means automating a lot of different things that need to talk to each other. This will always include a lot of different automated tasks and will generally involve configuring a bunch of different systems. Taking the example of the website infrastructure that we saw in our last video, we've seen how we can automate the creation of each instance in the system. Now, say you wanted to deploy a new copy of the system in a separate data center where you have no instances yet, you'll need to also automate the whole configuration of the system, the different instance types involved, how will each instance finesse the others, what the internal network looks like, and so on. So how does this work? **The key here is that the configuration of the overall system needs to be automatically repeatable.** There's a bunch of different tools that we can use to do that. **These tools typically don't communicate with the Cloud systems through the web interface or the command line.** **They normally use an application programming interface or API that lets us interact with the Cloud infrastructure directly from our scripts.** We'll talk more about other APIs in a later course. In the case of Cloud provider APIs, they typically let you handle the configuration that you want to sit directly from your scripts or programs without having to call a separate command. This combines **the power of programming with all of the available Cloud resources. The APIs offered by the Cloud providers let us perform all the tasks that we mentioned earlier like creating, modifying, and deleting instances and also deploying complex configurations for how these instances will talk to each other.** All of these actions can also be completed through the web interface or the command line. But doing them from our programs gives us extra flexibility which can be key when automating complex setups. **Say you wanted to deploy a system that combines some services running on a Cloud provider and some services running on-premise, this is known as a hybrid Cloud setup, or only part of the services are in the Cloud.** The setup is super common in the industry right now. Orchestration tools can be a pretty useful tool to make sure that both the on-premise services and the Cloud services know how to talk to each other and are configured with the right settings. Going back to the website example that we discussed earlier to make sure that the service is running smoothly, **we should set up a monitoring and alerting**. This lets us detect and correct any problems with our service before users even notice. This is a critical piece of infrastructure but setting it up correctly can take quite some time. **By using orchestration tools, we can automate the configuration of any monitoring rules** that we need to set, which metrics we want to look for, when we want to be alerted, and so on, and automatically apply these to a complete deployment no matter which datacenter the services are running in. This might seem like a super complex task, but fortunately there are tools available to make our lives easier. We'll talk about those in our next video.

**Automation is when we set up a single step in a process to require no oversight, while orchestration refers to automating the entire process.**

### Cloud Infrastructure as Code

In our last video, we talked about how we need to orchestrate complex Cloud setups. This includes handling a bunch of different nodes with different workloads, managing the complexity of deploying a hybrid setup, or modifying deployments across several Data centers. Back at the beginning of the course, we talked about infrastructure as Code, and we called out that storing our infrastructure in a code like format, lets us create repeatable infrastructure, and that using Version control for the storage, means that we can keep a history of what we've done and easily rollback mistakes. These principles also apply to Cloud infrastructure. The way we store it might be a little different depending on the tools that we use, but we'll still be storing this configuration in a code like format using Version control to keep track of the changes. This lets us manage large-scale solutions with a small team. We can very quickly have an idea of what the deployment looks like, by looking at the configuration. We can try new things out and roll back if anything goes wrong. We can look at the history of changes to figure out why a specific change was made, and much more. **Most Cloud providers offer their own tool for managing resources as code. Amazon has Cloud Formation, Google has Cloud Deployment Manager, Microsoft has Azure Resource Manager, and OpenStack has Heat Orchestration Templates.** These tools are specific to the Cloud provider, which means it can be complex and cumbersome to move to a different provider or combine a Cloud deployment with an on-premise deployments. **An option that's becoming really popular in the Orchestration field, is called Terraform.** Similar to Puppet, **Terraform uses its own Domain-specific language which lets us specify what we want our Cloud infrastructure to look like.** The cool thing about **Terraform is that it knows how to interact with a lot of different Cloud providers and automation vendors.** So you can write your Terraform rules to deploy something on one Cloud provider, and then use very similar rules to deploy the service to a different Cloud provider. Terraform uses each Cloud provider's API to accomplish this. This keeps you from having to learn a new API when moving to a different Cloud provider, and lets you focus on the infrastructure design. We saw in earlier videos how we can have a puppet rule that specifies that a computer should install a given package, and that the local puppet agent analyzes the computer and decides which installation mechanism to use depending on the operating system, the specific Linux distro and so on. A similar thing happens with Terraform. The rules that define the resources like the VMs or containers to use, will use specific values related to the Cloud provider like selecting which machine type to use or in what region to deploy it. But a lot of the overall configuration is independent of the provider, and can be reused if we decide to move our configuration to a different provider or we want to use a hybrid setup. Of course Terraform isn't the only option. Puppet itself also ships with a bunch of plug-ins that can be used to interact with the different Cloud providers to create and modify the desired Cloud infrastructure. Finally, let's spend a moment talking about the contents of the nodes or instances managed by the Orchestration tools. When dealing with nodes in the Cloud, there are basically two options. Either they're long-lived and their contents need to be periodically updated, or they are short-lived and updates are made by deleting the old instances and deploying new ones. Long-lived instances are typically servers that are not expected to go away. Things like your company's internal mail server or internal document sharing servers, will manage these instances using a configuration management system like Puppet, which can deploy any necessary changes to the machines while they're running. This keeps them updated to the latest state. On the flip side, short-lived instances come and go very quickly. For these cases, it makes less sense to apply changes while they're running. Instead, we normally apply the configuration that we want the instances to have when they start, and we deploy any future changes by replacing the instances with new ones. We can still use Puppet for the initial setup, but we don't need to run the agent periodically, only at the start. If all this sounds super complex, that's okay. There's a lot to learn about Cloud Orchestration, and many of these concepts will make more sense once you've tried them out. Up next, we've gathered some additional info you can use if you want to find out more. Then there is a quick quiz to put all of these concepts together.

 **IaC uses special machine-readable config files to automate configuration management.**


 ### Links

Check out the following links for more information:

Getting started on GCP with Terraform
Creating groups of unmanaged instances
Official documentation is here: https://cloud.google.com/load-balancing/docs/https/
https://geekflare.com/gcp-load-balancer/
Interesting articles about hybrid setups:

https://blog.inkubate.io/create-a-centos-7-terraform-template-for-vmware-vsphere/
https://www.terraform.io/docs/enterprise/before-installing/reference-architecture/gcp.html
https://www.hashicorp.com/resources/terraform-on-premises-hybrid-cloud-wayfair

### Module 3 Wrap Up: Automation in the Cloud

Over the past few videos we've learned how to use the different Cloud resources available to us. We've gone through a bunch of different concepts like software or infrastructure-as-a-service, public or hybrid clouds, upscaling and downscaling, and a lot more. We've also demonstrated how to deploy single virtual machines and then turn them into a customized VM template. Creating a single VM can be useful for small to medium-sized organizations with lower technical requirements. But as the technical requirements for the organization grows, it's often necessary to deploy more and larger Cloud solutions. This is where using the template to create large system clusters becomes very handy. Using reference images and templating lets us clone a VM 100, 1,000, or more times, and this makes scaling our Cloud deployments super easy. We checked out a bunch of different ways to interact with the platform. We've seen how we can use both the web interface and the command line tool to create virtual machines in the Cloud. Using these tools we can control which machines are online or offline, modify their configuration, and a bunch of other things. At a small or medium scale, using these tools can be really effective. At a larger scale, we have to automate these deployments even further and that's where orchestration comes into play. Tools like Terraform let us define our Cloud infrastructure as code, allowing us to have a lot of control over how the infrastructure is managed, how the changes are applied, and so on. This lets us combine the power of using infrastructure as code with the flexibility of using Cloud resources. Hopefully by now, you're starting to see how you can make the best out of the different Cloud offerings to help your IT infrastructure quickly and easily scale as needed. Up next, you'll have the opportunity to practice all of this in the next lab. Instead of relying on the Quick Labs infrastructure to set up all the VMS for you, you'll be doing the setup yourself. Exciting, right?

## Managing Cloud Instances at Scale

### Storing Data in the Cloud

Almost all IT systems need to store some data. Sometimes, it's a lot of data, sometimes, it's only bits and pieces of information. Cloud providers give us a lot of storage options. Picking the right solution for data storage will depend on what service you're building. You'll need to consider a bunch of factors, like; how much data you want to store, what kind of data that is, what geographical locations you'll be using it in, whether you're mostly writing or reading the data, how often the data changes, or what your budget is. This might sound like a lot of things to consider, but don't worry, it's not that bad. We'll check out some of the most common solutions offered by Cloud providers to give you a better idea of when to choose what. When choosing a storage solution in the Cloud, you might opt to go with the traditional storage technologies, like block storage, or you can choose newer technologies, like object or blob storage. Let's check out what each of these mean. **As we saw in an earlier video, when we create a VM running in the Cloud, it has a local disk attached to it. These local disks are an example of block storage.** This type of storage closely resembles the physical storage that you have on physical machines using physical hard drives. Block storage in the Cloud acts almost exactly like a hard drive. **The operating system of the virtual machine will create and manage a file system on top of the block storage just as if it were a physical drive.** There's a pretty cool difference though. These are virtual disks, so we can easily move the data around. For example, we can migrate the information on the disk to a different location, attach the same disk image to other machines, or create snapshots of the current state. All of this without having to ship a physical device from place to place. **Our block storage can be either persistent or ephemeral.** **Persistent storage is used for instances that are long lived, and need to keep data across reboots and upgrades.** On the flip side, ephemeral storage is used for instances that are only temporary, and only need to keep local data while they're running. **Ephemeral storage is great for temporary files that your service needs to create while it's running, but you don't need to keep.** This type of storage is especially common when using containers, but it can also be useful when dealing with virtual machines that only need to store data while they're running. **In typical Cloud setups, each VM has one or more disks attached to the machine.** The data on these disks is managed by the OS and can't be easily shared with other VMs. If you're looking to share data across instances, you might want to look into some shared file system solutions, that Cloud providers offer using the platform as a service model. **When using these solutions, the data can be accessed through network file system protocols like NFS or CIFS.** This lets you connect many different instances or containers to the same file system with no programming required. Block storage and shared file systems work fine when you're managing servers that need to access files. **But if you're trying to deploy a Cloud app that needs to store application data, you'll probably need to look into other solutions like objects storage, which is also known as blob storage.** **Object storage lets you place in retrieve objects in a storage bucket.** These objects are just generic files like photos or cat videos, encoded and stored on disk as binary data. **These files are commonly called blobs, which comes from binary large object**, and as we called out, **these blobs are stored in locations known as buckets**. Everything that you put into a storage bucket has a unique name. There's no file system. You place an object into storage with a name, and if you want that object back, you simply ask for it by name. **To interact with an object store, you need to use an API or special utilities that can interact with the specific object store that you're using**. On top of this, we've called out in earlier videos that most Cloud providers offer databases as a service. **These come in two basic flavors, SQL and NoSQL**. **SQL databases, also known as relational, use the traditional database format and query language. Data is stored in tables with columns and rows that can be indexed, and we retrieve the data by writing SQL queries.** A lot of existing applications already use this model, so it's typically chosen when migrating an existing application to the Cloud. **NoSQL databases offer a lot of advantages related to scale.** They're designed to be distributed across tons of machines and are super fast when retrieving results. **But instead of a unified query language, we need to use a specific API provided by the database.** This means that we might need to rewrite the portion of the application that accesses the DB. When deciding how to store your data, you'll also have to choose a storage class. Cloud providers typically offer different classes of storage at different prices. Variables like performance, availability, or how often the data is accessed will affect the monthly price. **The performance of a storage solution is influenced by a number of factors, including throughput, IOPS, and latency.** Let's check out what these mean. **Throughput is the amount of data that you can read and write in a given amount of time.** The throughput for reading and writing can be pretty different. For example, you could have a throughput of one gigabyte per second for reading and 100 megabytes per second for writing. **IOPS or input/output operations per second measures how many reads or writes you can do in one second, no matter how much data you're accessing.** Each read or write operation has some overhead. So there's a limit on how many you can do in a given second, and **latency is the amount of time it takes to complete a read or write operation**. This will take into account the impact of IOPS, throughput and the particulars of the specific service. **Read latency is sometimes reported as the time it takes a storage system to start delivering data after a read request has been made, also known as time to first byte**. While **write latency is typically measured as the amount of time it takes for a write operation to complete**. When choosing the storage class to use, you might come across terms like hot and cold. **Hot data is accessed frequently and stored in hot storage while cold data is accessed infrequently, and stored in cold storage**. These two storage types have different performance characteristics. For example, **hot storage back ends are usually built using solid state disks, which are generally faster than the traditional spinning hard disks**. So how do you choose between one and the other? Say you want to keep all the data you're service produces for five years, but you don't expect to regularly access data older than one year. You might choose to keep the last one year of data in hot storage so you have fast access to it, and after a year, you can move your data to cold storage where you can still get to it, but it will be slower and possibly costs more to access. There's a lot more to say about storage in the Cloud. It's really quite a hot topic, but we won't go into more detail here. We'll provide links to more info in the next reading in case you want to learn more. Up next, we're going to look at a different Cloud scale characteristic that we've already touched on using multiple machines for the same service.

### Load Balancing

In earlier videos, we saw a bunch of different reasons why we might want more than one machine or container running our service. For example, we might want to horizontally scale our service to handle more work, distribute instances geographically to get closer to our users. Or have backup instances to keep the service running if one or more of the instances fail. No matter the reason, we use orchestration tools and techniques to make sure that the instances are repeatable. And once we've set up replicated machines, we'll want to distribute the requests across instances. We called out earlier that this is where load balancing comes into play. Let's take a closer look at the different load balancing methods that we can use. A pretty common load balancing technique is **round robin DNS. Round robin is a really common method for distributing tasks. Imagine you're giving out treats at a party.** First, you make sure that each of your friends gets one cookie. Then you give everyone a second serving and so on until all of the treats are gone or your guests say, thank you, they're full. That's the round-robin approach to eating all the cookies. Now, if we want to translate a URL like my service.example.com into an IP address, we use the DNS protocol or domain name system. In the simplest configuration, the URL always gets translated into exactly the same IP address. But **when we configure our DNS to use round robin, it'll give each client asking for the translation a group of IP addresses in a different order. The clients will then pick one of the addresses to try to reach the service. If an attempt fails, the client will jump to another address on the list.**

This load balancing method is super easy to set up. You just need to **make sure that the IPs of all machines in the pool are configured in your DNS server**, but it has some limitations. First, **you can't control which addresses get picked by the clients**. Even if a server is overloaded, you can't stop the clients from reaching out to it. On top of that, DNS records are cached by the clients and other servers. So if you need to change the list of addresses for the instances, you'll have to wait until all of the DNS records that were cached by the clients expire. There's got to be a better way, right? Well, there sure is. **To have more control over how the load's distributed and to make faster changes, we can set up a server as a dedicated load balancer.** **This is a machine that acts as a proxy between the clients and the servers.** **It receives the requests and based on the rules that we provide, it directs them to the selected back-end server.**

Load balances can be super simple or super complex depending on the service needs. **Say your service needs to keep track of the actions that a user has taken up till now. In this case, you'll want your load balancer to use sticky sessions.** Using **sticky sessions means all requests from the same client always go to the same back end server**. This can be really useful for services than need it but can also cause headaches when migrating or maintaining your service. So you need to use it only if you really need it. Otherwise, you'll end up in a really sticky situation. **Another cool feature of load balancers is that you can configure them to check the health of the backend servers.** Typically, we do this by making a simple query to the servers and checking that the reply matches the expected reply. If a back-end server is unhealthy, the load balancer will stop sending new requests to it to keep only healthy servers in the pool. As we've called out a few times already, a cool feature of cloud infrastructure is how easily we can add or remove machines from a pool of servers providing a service. If we have a load balancer controlling the load of the machines, adding a new machine to the pool is as easy as creating the instance. And then letting the load balancer know that it can now route traffic to it. We can do this by manually creating and adding the instance or when our services under heavy load, we can just let the auto scaling feature do it. Cool, right? Okay, so imagine that you've built out your service with load balancers and you're receiving requests from all over the world.

How do you **make sure that clients connect to the servers that are closest to them? You can use Geo DNS and geoip**.

These are DNS configurations that will direct your clients to the closest geographical load balancer. The mechanism used to route the traffic relies on how the DNS servers respond to requests. For example, from machines hosted in North America, a DNS server in North America might be configured to respond with the IPs in, you guessed it, North America. It can be tricky to set this up on your own but most Cloud providers offer it as part of their services making it much easier to have a geographically distributed service. Let's take this one step further. **There are some providers dedicated to bringing the contents of your services as close to the user as possible. These are the content delivery networks or CDNs.** **They make up a network of physical hosts that are geographically located as close to the end user as possible.**

This means that CDN servers are often in the same data center as the users Internet service provider. CDNs work by caching content super close to the user. When a user requests say, a cute cat video, it's stored in the closest CDN server. That way, when a second user in the same region requests the same cat video, it's already cached in a server that's pretty close and it can be downloaded extra fast. Because no one should have to wait for their cat videos to load.

You now know a lot of stuff about load-balancing. Up next, we'll talk about another aspect of dealing with cloud services. How to deploy changes safely to our cloud service?

### Change Management

You've come a long way. You now know how to get your service running in the cloud. Next, let's talk about how to keep it running. Most of the time when something stops working, it's because something changed. If we want our cloud service to be stable, we might be tempted to avoid changes altogether. But change is a fact of cloud life. If we want to fix bugs and improve features in our services, we have to make changes. But we can make changes in a controlled and safe way. This is called **change management, and it's what lets us keep innovating while our services keep running**. **Step one in improving the safety of our changes, we have to make sure they're well-tested. This means running unit tests and integration tests, and then running these tests whenever there's a change**. In an earlier course, we briefly mentioned continuous integration, or CI, but here's a refresher. **A continuous integration system will build and test our code every time there's a change.**

Ideally, the CI system runs even for changes that are being reviewed. That way you can catch problems before they're merged into the main branch. You can use **a common open source CI system like Jenkins**, or if you use **GitHub, you can use its Travis CI integration**. Many cloud providers also offer continuous integration as a service. Once the change has committed, the CI system will build and test the resulting code. Now you can use **continuous deployment, or CD, to automatically deploy the results of the build or build artifacts.** Continuous deployment lets you control the deployment with rules. For example, we usually configure our CD system to deploy new builds only when all of the tests have passed successfully.

On top of that, we can configure our CD to push to different environments based on some rules. What do we mean by that? In an earlier video, we mentioned that when pushing puppet changes, we should have a test environment separate from the production environment.

Having them separate lets us validate that changes work correctly before they affect users. Here environment means everything needed to run the service. It includes the machines and networks used for running the service, the deployed code, the configuration management, the application configurations, and the customer data. Production, usually shortened to prod, is the real environment, the ones users see and interact with. Because of this, we have to protect, love, and nurture a prod. The test environment needs to be similar enough to prod that we can use them to __check our changes work correctly. You could have your CD system configured to push new changes to the test environment. You can then check that the service is still working correctly there, and then manually tell your deployment system to push those same changes to production__.

* If the service is complex and there are a bunch of different developers making changes to it, you might set up additional environments where the developers can test their changes in different stages before releasing them. For example, you might have your CD system push all new changes to a development or dev environment, then have a separate environment called pre-prod, which only gets specific changes after approval. And only after a thorough testing, these changes get pushed to pro. Say you're trying to increase the efficiency of your surface by 20%, but you don't know if the change you made might crash part of your system.

You want to deploy it to one of those testing or development environments to make sure it works correctly before you ship it to prod.

Remember, these environments need to be as similar to prod as possible. They should be built and deployed in the same way. And while we don't want them to be breaking all the time, it's normal for some changes to break dev or even pre-prod. We're just happy that we can catch them early so that they don't break prod. Sometimes you might want to experiment with a new service feature. You've tested the code, you know it works, but you want to know if it's something that's going to work well for your users. When you have something that you want to test in production with real customers, you can experiment using A/B testing. **In A/B testing, some requests are served using one set of code and configuration, A, and other requests are served using a different set of of code and configuration, B.** This is another place where a load balancer and instance groups can help us out. You can deploy one instance group in your A configuration and a second instance group in your B configuration. Then by changing the configuration of the load balancer, you can direct different percentages of inbound requests to those two configurations. If your A configuration is today's production configuration and your B configuration is something experimental, you might want to start by only directing 1 % of your requests to B. Then you can slowly ramp up the percentage that you check out whether the B configuration performs better than A, or not. Heads up, make sure you have basic monitoring so that it's easy to tell if A or B is performing better or worse. If it's hard to identify the back-end responsible for serving A requests or B requests, then much of the value of A/B testing is lost to A/B debugging. So what happens if all the precautions we took aren't enough and we break something in production? Remember what we discussed in an earlier course about post-mortems. We learn from failure and we build the new knowledge into our change management.

Ask yourself, what did I have to do to catch the problem? Can I have one of my change management systems look for problems like that in the future? Can I add a test or a rule to my unit tests, my CI/CD system, or my service health checks to prevent this kind of failure in the future? So remember, if something breaks, give yourself a break. Sometimes in IT, these things happen, no matter how careful you are. And as you use and refine your change management systems and skills, you'll gain the confidence to make changes to your service more quickly and safely.

### Understanding Limitations

We've spent a while talking about how to make your service runs smoothly in the Cloud, now let's take a moment to talk about some of the problems that you might come across. Personally, I find that when writing software to run on the Cloud, it's important to keep in mind how my application will be deployed. The software I'm creating needs to be fault tolerant and capable of handling unexpected events. Instances might be added or removed from the pool as needed and if an individual machine crashes, my service needs to breeze along without introducing problems, and not every problem results in a crash, **sometimes we run into quotas or limits, meaning that you can only perform a certain number of operations within a certain time period.** For example, when using Blob Storage there might be a limit of 1,000 writes to the same blob in a given seconds. If your service performs a lot of these operations routinely, it might get blocked by these limits. In that case, you'll need to see if you can change the way you're doing the operations, for example by grouping all of the calls into one batch. Switching to a different service is sometimes an option too. Some API calls used in Cloud services can be expensive to perform, so **most Cloud providers will enforce rate limits on these calls to prevent one service from overloading the whole system**. For example, there might be a rate limit of one call per second for an expensive API call. On top of that, there are also **utilization limits, which cap the total amount of a certain resource that you can provision.** These quotas are there to help you avoid unintentionally allocating more resources than you wanted. Imagine you've configured your service to use auto scaling and it suddenly receives a huge spike in traffic. This could mean a lot of new instances getting deployed which can cost a lot of money. __For some of these limits, you can ask for a quota increase from the Cloud provider if you want additional capacity, and you can also set a smaller quota in the default to avoid overspending.__ This can be a great idea when you're running a service on a tight budget. If your service performance expensive operations routinely, you should make sure you understand the limitations of the solution that you choose. A lot of platform as a service and infrastructure as a service offerings have costs directly related to how much they're used. They also have usage quotas. If the service you've built suddenly becomes very popular, you can run out of quota or run out of budget. By imposing a quota on an auto-scaling system, the system will grow to meet user demand until it reaches the configured limit. The trick here is to have good monitoring and alerting around behavior like this. If your system runs out of quota but there's an increased demand for a puppy videos, the system may have problems, degraded performance or worse yet an outage. So you want to be notified as soon as it happens that you can decide whether to increase your quota or not. Finally, let's talk about dependencies. When your service depends on a Platform as a Service offering like a hosted database or CICD system, you're handing the responsibility for maintenance and upgrades of that service off to your Cloud provider, that's great, fewer things to worry about and maintain, but it also means that you don't always get to choose what version of that software you're using. You might find yourself on either side of the upgrade cycle, either wanting to stay at a version that's working well for you or wanting the Cloud provider to hurry up and upgrade to resolve a bug that's affecting your service. Your Cloud provider has a strong incentive to keep its service software fairly up-to-date. Keeping software as a service solutions up to date ensures that customers aren't vulnerable to security flaws, that bugs are promptly fixed and that new features get released early. At the same time, the Cloud provider has to move carefully and test changes to keep destruction of its service to a minimum. They will communicate proactively about changes to the services that you use and in some cases, Cloud providers might give you access to early versions of these services. For example, you can set up a test environment for your service that uses the beta or prerelease version of a given software as a service solution, letting you test it before it impacts production. Hopefully, you're starting to get an idea of the trade-offs that you'll need to make to get the most from deploying your software to the Cloud. Now, head on over to the reading for links to additional information on all of this, then there's a quick quiz for you.

### Links

Here are some links to some common Quotas youll find in various cloud providers

https://cloud.google.com/compute/quotas#understanding_vm_cpu_and_ip_address_quotas
https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html
https://docs.microsoft.com/en-us/azure/azure-subscription-service-limits#service-specific-limits

### Getting Started with Monitoring

As we called out in an earlier video, once we have our service running in the Cloud, we want to make sure that our service keeps running, and not just that, we want to make sure it keeps behaving as expected, returning the right results quickly and reliably. The key to ensuring all of this, is to set up good monitoring and alerting rules. In the next few videos, we'll do a rundown of monitoring and alerting concepts and techniques, followed by a practical demonstration. Let's dive in. To understand how our service is performing, we need to monitor it. **Monitoring lets us look into the history and current status of a system.** How can we know what the status is? We'll check out a bunch of different metrics. **These metrics tell us if the service is behaving as expected or not. Well, some metrics are generic, like how much memory an instance is using.** Other metrics are specific to the service we want to monitor. Say your company is running a website and you want to check if it's working correctly. **When a web server responds to an HTTP request, it starts by sending a response code, followed by the content of the response.** You might know, for example, that a 404 code means that the page wasn't found, or that a 500 response means that there was an internal server error. In general, response codes in the 500 range, like 501 or 503, tells us that something bad happened on the server while generating a response. Well, response codes in the 400 range means there was a client-side problem in the request. When monitoring your web service, you want to check both the count of response codes and their types to know if everything's okay. If you're running an e-commerce site, you'll care about how many purchases were made successfully and how many failed to complete. If you're running a mail server, you want to know how many emails were sent and how many got stuck and so on. You'll need to think about the service you want to monitor and figure out the metrics you'll need. Now, once we've decided what metrics we care about, what do we do with them? We'll typically store them in the monitoring system. **There's a bunch of different monitoring systems out there. Some systems like AWS Cloudwatch, Google Stack Driver, or Azure Metrics are offered directly by the Cloud providers.** **Other systems like Prometheus, Datadog, or Nagios can be used across vendors.** There's two ways of getting our metrics into the monitoring system. **Some systems use a pull model, which means that the monitoring infrastructure periodically queries our service to get the metrics. Other monitoring systems use a push model, which means that our service needs to periodically connect to the system to send the metrics.** No matter how we get the metrics into the system, we can create dashboards based on the collected data. This dashboard show the progression of the metrics over time. We can look at the history of one specific metric to compare the current state to how it was last week or last month. Or we can look at the progression of two or more metrics together to check out how the change in one metrics effects another. Imagine it's Monday morning and you notice that your service is receiving a lot less traffic than usual. You can look at the data from past weeks and see if you always get less traffic on Monday mornings or if there's something broken causing your service to be unresponsive. Or if you see that in the past couple days, the memory used by your instances has been going up, you can check if this growth follows a similar increase in another metric, like the amount of requests received or the amount of data being transmitted. This can help you decide if there's been a memory leak that needs to be fixed or if it's just an expected consequences of a growth in popularity. **Pro tip, you only want to store the metrics that you care about, since storing all of these metrics in the system takes space, and storage space costs money.** When we collect metrics from inside a system, like how much storage space the service is currently using or how long it takes to process a request, this is called whitebox monitoring. **Whitebox monitoring checks the behavior of the system from the inside.** We know the information we want to track, and we're in charge of making it possible to track. For example, if we want to track how many queries we're making to the database, we might need to add a variable to count this. On the flip side, **blackbox monitoring checks the behavior of the system from the outside**. This is typically done by making a request to the service and then checking that the actual response matches the expected response. We can use this to do a very simple check to know if the service is up and to verify if the service is responding from outside your network. Or we could use it to see how long it takes for a client in a different part of the world to get a response from the system. Okay, monitoring is really cool, but who wants to stare at dashboards all day trying to figure out if something's wrong? Fortunately, we don't have to. Instead, we can set up alerting rules to let us know if something's wrong. This is a critical part of ensuring a reliable system, and we're going to learn how to do it in the next video.

**When push monitoring is used, the service being monitored actively sends metrics to the monitoring system.**

### Getting Alerts When Things Go Wrong

We expect a lot from our modern IT services. We expect them to be up and running 24-7. We want to be able to get our work done whenever and wherever. For that, we need our services to respond day or night, workday or holiday. But even if the services are running 24-7, System Administrators can't constantly be in front of their systems. Instead, we set up our services so that they work unattended and deal with problems when they happen. Now to do this, we need to detect those problems so that we can deal with them as quickly as possible. If you have no automated way of raising an alert, you might only find out about the issue when you get a call from a frustrated user telling you that your service is down. That's not ideal. It's much better to create automation that checks the health of your system and notifies you when things don't behave as expected. This can give you advance warning that something's wrong, sometimes even before users notice a problem at all. So how do we do that? The most basic approach is to run a job periodically that **checks the health of the system and sends out an email if the system isn't healthy**. **On a Linux system, we could do this using cron, which is the tool to schedule periodic jobs.** We'd pair this with a simple Python script that checks the service and sends any necessary emails. This is an extremely simplified version of an alerting system, but it shares the same principles. Is all alerting systems, no matter how complex and advanced. We want to periodically check the state of the service and raise alerts if there's a problem. When you use a monitoring system like the ones we described in our last video, the metrics you collect represent the state of your service. Instead of periodically running a script that connects to the service and checks if it's responding, you can configure the system to periodically evaluate the metrics; and based on some conditions, decide if an alert should be raised. **Raising an alert signals that something is broken and a human needs to respond.** For example, you can set up your system to raise alerts if the application is using more than 10 gigabytes of RAM, or if it's responding with too many 500 errors, or if the queue of requests waiting to get processed gets too long. Of course, not all alerts are equally urgent. **We typically divide useful alerts into two groups, those that need immediate attention and those that need attention in the near future.** If an alert doesn't need attention, then it shouldn't have been sent at all. It's just noise. If your web service is responding with errors to 50 percent of the requests, you should look at what's going on right away. Even if this means waking up in the middle of the night to address whatever is wrong, you'll definitely want to fix this kind of critical problem ASAP. On the other hand, if the issue is that the attached storage is 80 percent full, you need to figure out whether to increase the disk size or maybe clean up some of the stored data. But this isn't super urgent, so don't let it get in the way of a good night's sleep. Since these two types of alerts are different, we typically configure our systems to raise alerts in two different ways. **Those that need immediate attention are called pages**, which comes from a device called a pager. Before mobile phones became popular, pagers were the device of choice for receiving urgent messages, and they're still used in some places around the world. Nowadays, most people receive their pages in other forms like SMS, automated phone calls, emails, or through a mobile app, but we still call them pages. On the flip side, the **non-urgent alerts are usually configured to create bugs or tickets** for an IT specialist to take care of during their workday. They can also be configured to send email to specific mailing lists or send a message to a chat channel that will be seen by the people maintaining the service. One thing to highlight is that **all alerts should be actionable**. If you get a bug or a page and there's nothing for you to do, then the alert isn't actionable and it should be changed or it shouldn't be there at all. Otherwise, it's just noise. Say you're trying to check if your services database back-end is responsive. If you do this by creating a query that returns all rows in a large table, your request might sometimes timeout and raise an alert. That would be a noisy alert, not really actionable. You'd need to tweak the query to make the check useful. Say you run a cron job that copies files from one location to another every 10 minutes, you want to check that this job runs successfully. So you configure your system to alert you if the job fails. After putting this in production, you realize there's a bunch of unimportant reasons that can cause this job to temporarily fail. Maybe the destination storage is too busy and so sometimes the job times out. Maybe the origin was being rebooted right when the job started, so the job couldn't connect to it. No matter why, whenever you go to check out what caused a job to fail, you discover that the following run had succeeded and there's nothing for you to do. You need to rethink the problem and tweak your alert. Since the task is running frequently, you don't care if it fails once or twice, you can change the system to only raise the alert if the job fails three times in a row. That way when you get a bug, it means that it's failing consistently and you'll actually need to take action to fix it. All of this configuring and tweaking can seem like a lot of work. You need to think about which metrics you care about. Configure your monitoring system to store them, then configure your alerting system to raise alerts when things don't behave as expected. The flip side is that once you've set your systems to raise actionable alerts when needed, you're going to have peace of mind. If no alerts are firing, you know the service is working fine. This lets you concentrate on other tasks without having to worry. To set up good alerts, we need to figure out which situations should page, which ones should create bugs, and which ones we just don't care about. These decisions aren't always easy and might need some discussion with the rest of your team. But it can help make sure that you spend time only on things that actually matter. Up next, we'll talk about criteria that we can use to decide which situation should raise alerts and what to do about them.

### Service-Level Objectives

We all know that some IT systems are more critical than others. Let's be real, if you try to play a computer game that you haven't opened in a year and it doesn't work, you probably won't care as much as if you're trying to make a bank transfer and your bank's website is down. **Sometimes a piece of infrastructure can be down and the overall system still works with degraded performance.** For example, if the caching server that makes your web application go faster is down, the app can still function, even if it's running slower. No system is ever available 100% of the time, it's just not possible. But **depending on how critical the service is, it can have different service level objectives, or SLOs**. **SLOs are pre-established performance goals for a specific service. Setting these objectives helps manage the expectations of the service users, and the targets also guide the work of those responsible for keeping the service running. SLOs need to be measurable, which means that there should be metrics that track how the service is performing and let you check if it's meeting the objectives or not.**

Many SLOs are expressed as how much time a service will behave as expected. For example, a service might promise to be available 99% of the time.

Heads up, when dealing with metrics and availability, we need to do a little math to understand what those numbers mean in practice, but don't worry, it's all pretty straightforward. If our service has an SLO of 99% availability, it means it can be down up to 1 % of the time. If we measure this over a year, the service can be down for a total of 3.65 during the year and still have 99% availability. **Availability targets like this one are commonly named by their number of nines.** Our 99% example would be a two 9 service, 99.9% availability is a three 9 service, 99.999% availability is a five 9 service. Five nine services promised a total down time of up to five minutes in a year. Five nines is super high availability, reserved only for the most critical systems. A three nine service, aiming for a maximum of eight hours of downtime per year, is fine for a lot of IT systems. Now, you might be wondering, why not just make everything a five nine service? It's a good question. The answer is, because it's really expensive and usually not necessary. If your service isn't super critical and it's okay for it to be down briefly once in a while having two or three nines of availability might be enough. You can keep the service running with a small team.

Five nine services usually require a much larger team of engineers to maintain it. Any service can have a bunch of different service level objectives like these, they tell its users what to expect from it. Some services, like those that we pay for, also have more strict promises in the form of service level agreements, or **SLAs. A service level agreement is a commitment between a provider and a client.**

Breaking these promises might have serious consequences. Service level objectives though are more like a soft target, it's what the maintaining team aims for, but the target might be missed in some situations. As we called out, having explicit SLOs or SLAs is useful for both the users of that service and the team that keeps the service running. If you're using a cloud service, you can decide how much you're going to entrust your infrastructure to it, based on the SLAs that the provider publishes. If on the other hand you're part of the team that maintains the service, you can **use the SLOs and SLAs of your service to decide which alerts to create and how urgent they should be.** Say you have a service with an SLO that says that at least 90% of the requests should return within 5 seconds. To know if your service is behaving correctly, you need to measure how many of the total requests are returning within those 5 seconds, and you want that number to always be above 90%. So you might set up a non-paging alert to notify you if less than 95% return within 5 seconds, and a paging alert if less than 90% return promptly. If you're in charge of a website, you'll typically measure the rate of responses with 500 return codes to check if your service is behaving correctly. If your SLO is 99% of successful requests, you can set up a non-paging alert if the rate of errors is above 0.5%, and a paging alert if it reaches 1%. In an earlier video, we called out that services usually break because something changed. That's also often the case when looking at what makes services go out of SLO. 
* If your service was working fine and meeting all of its SLOs and then started misbehaving, it's likely this was caused by a recent change. That's why some teams use the concepts of **error budgets** to handle their services.

Say you're running a service that has three nines of availability. This means the service can be down 43 minutes per month, this is your error budget. You can tolerate up to 43 minutes of downtime, so you keep track of the total time the service was down during the month. If it starts to get close to those 43 minutes, you might decide to stop pushing any new features and focus on resolving the problems that keep causing the downtime. Now, all this talk of nines, availability and downtime can have your head spinning if you've never done this before, and that's totally normal. **If it's your first time setting objectives for your service, start by setting achievable goals that you can measure.**

**Track how the service behaves for a while and see what causes the service to deviate from the targets.** Once you have a better idea of the whole service's behavior, you can set more aggressive goals. Up next, we'll go back to our VMS running in the cloud and demonstrate how we can monitor them using the tools offered by the provider.

### Basic Monitoring in GCP

So far, we've seen how to create virtual machines in the Google Cloud Console. We've kept these virtual machines running and now we want to see how we can use the tools provided by the cloud vendor to monitor them and create alerts based on them. For this demonstration, we'll use the **monitoring tool called Stackdriver, which is part of the overall offering**. When you first activate this system, it takes a while until it starts collecting on the metrics from all the machines, so we've activated in advance.

When we first opened the monitoring console, we see an overview of the system. At the moment, this is looking pretty empty, but we could configure this dashboard to show the charts that we consider the most useful. Let's go into the instances dashboard, we see here the list of our instances and we can click on each of them to see that monitoring information that Stackdriver has collected about them. The monitoring system gives us a very simple overview of each of the instances with three basic metrics, CPU usage, Disk I/O, and network traffic. Depending on what surface we want to run on a VM, we can customize these dashboards to show different metrics. If the metrics that come baked in aren't enough, you can create your own metrics and also add them here. Now we want **to check out how to set up an alert to notify us if something isn't behaving correctly**. To do this, **we'll create a new alerting policy. To set up a new alert, we have to configure the condition that triggers the alert**. After we've done that, we can also configure how we want to be notified of the issue and add any documentation that we want the notification to include. Let's start by configuring the condition. As we called out, **alerting conditions are related to specific metrics.** We want to be notified when the metric indicates that there's a problem with an instance. For this example, we're going to configure an alert that triggers if an instance in CPU utilization is more than 90 percent. We'll start by selecting that we want to monitor GCE, VM instances, which are the instances that we currently have running and then select the CPU utilization metric.

After selecting the metric, we see the graph of the collected values for all of the current running instances. We can optionally add extra filters and groups for the data for this condition. For example, we could choose to only look at some of the instances, selecting by their zone, region, or name. This can be useful if you want to have separate alerts for instances used for production, and those used for testing or development. On top of that, we can also choose an aggregator for the data, **these aggregators are useful when the metrics that you're collecting are about the overall system and not just one instance**. For example, if you're checking the number of error responses that your system generated, you want to sum all the errors across instances. Depending on how we filter group and aggregate the data, we'll end up with a bunch of different time series, we'll use these values to decide if we should trigger the alert or not. The next step is **selecting how many of the different time series need to violate the condition for the alert to trigger**. We can trigger the alert when one, some, or all of the different time series violate the condition. For this example, we'll configure our alert to trigger if any instance is using more than 90 percent of the CPU. So, let's select any time series violates. Now, we'll say that we want our alert to trigger if the value is above 90 percent for one minute. All right. We've set the condition. Now, we can select how we want to get the notification and when the alert triggers. Currently, the only type of notification that we can use is e-mail. To use the other channel types available, we need to configure them in our profile. For this example, e-mail will do. Using e-mails can be just fine when you're getting started with alerting, but eventually you'll want to configure additional methods. All right. We've configured our alert to send e-mails. Now, we can add extra documentation to our alert. This documentation is intended to help the person that's responding to the alert understand what they need to do to fix the problem. Including good documentation here, it can be super-important when you've got a bunch of different people working together in a team and not everyone knows everything. Alerts that include good documentation are much easier to tend to and help get the service back to a healthy state faster. For our example, we'll add a message saying that whoever is taking care of this alert, should check the instance with top. Finally, we'll need to give a name to our alerting policy, we'll call it CPU and then save it.

Now, we've set up our alert. Now, we can sit back and relax knowing that if anything goes wrong, we'll be the first to know. For the final part of this demo, we want to show what happens when the alert triggers. To do that, we'll start a process in one of our instances that uses all of the available CPU, by creating an infinite loop. So we'll go back to the main console, SSH into the VM, called Linux- instance and then create a wire loop that never ends.

Now, our loop is running and using all of the available CPU, we can check this by running **the top command that shows us the CPU usage**. We see that there's a bash command that's using almost 100 percent of the available CPU, our experiment is working. Now, remember that we said that we wanted the condition to be true for a minute before the alert triggers, it won't trigger just yet. **It's common practice to use time windows of one, five, or even 10 minutes when dealing with the alerting.** We don't want to get an alert for a small spike that lasted only a few seconds and then went away. We want to get alerted when there's an actual problem that requires our attention. 
* The size of the time window we choose depends on the metric we're checking, the length of the expected spikes and a bunch of other factors. It's pretty normal to have to tweak how long we want the condition to be true as we try our alert out. If you're getting notified too often about conditions that go away on their own without you having to do anything, you might choose to make the time window larger. On the flip side, if you're getting notified too late about conditions that needed attention, you might choose to make the time window smaller.

All right. We've let enough time pass, let's check out what's up with our alert.

We see that there's an open incident, which is a way of grouping problems. The alerts summary gives us a bunch of info about what's going on. We can click on the CPU link to get more information. This page shows us the metric that triggered the alert for the incident, it shows the threshold for triggering the alert and the current value of the metric. It also shows us the documentation that we entered and lets us create annotations. We can use these annotations to track the work that we do during an incident. All right. Let's stop the process that's using all of our instances CPU. It's still running the top process from before. Let's exit with Q. Now, the infinite loop is currently running in the background of our console. We can make it run in the foreground by typing fg, and then cancel it by pressing CTRL+C. Now, we've stopped the process. Let's check with top that it's no longer using all of our CPU. Great, the bash process isn't taking all of the CPU time anymore. In another minute, the alert that we'd saw earlier will stop triggering, nice. With that, we've demonstrated how we can monitor a bunch of instances running in the Cloud. We've created an alert based on metrics and verify that the alert triggers. Of course, there's a lot more that we can do with these tools, we'll give you pointers to more information in the reading coming up. After that, there's another quick quiz to check that everything is making sense.

**An Alerting Policy specifies the conditions that trigger alerts, and the actions to be taken when these alerts are triggered, like sending an email address notification.**

### Links

Check out the following links for more information:

https://www.datadoghq.com/blog/monitoring-101-collecting-data/
https://www.digitalocean.com/community/tutorials/an-introduction-to-metrics-monitoring-and-alerting
https://en.wikipedia.org/wiki/High_availability
https://landing.google.com/sre/books/

### What to Do When You Can't Be Physically There

If you're used to troubleshooting problems on physical computers. It can take a bit of a mindset shift to get used to fixing problems with virtual machines running in the cloud. There's a bunch of things that you can't do, you can't walk up to a server and check out what's wrong with it. But there's also other things that are a lot simpler when troubleshooting VMs in the cloud. Like adding more memory or moving the machine to a different data center. Let's say that after the latest upgrade, a bunch of your cloud VMs have stopped booting. Something went wrong, but you don't know exactly what. You can't connect to the instances or boot them in rescue mode, so what can you do? There's a bunch of options, if you're following the infrastructure as code practices that we've talked about. You could deploy new VMs with the previous version of the system, this would help us get back to a healthy state as quickly as possible. On top of this, you want to understand the problem and how to fix it. To do that, you can create a snapshot of the disk image for one of the failing VMs. And then mount that disk image on a healthy machine. That way you can analyze the contents of the image and figure out what's causing the failures. And it's not always easy to know which piece of the system is causing a failure. Especially if the system is complex with many different services interacting with each other. If you're trying to figure out what's causing your complex servers to respond with a ton of 500s. You need to look at different pieces individually until you find the culprit. Does the problem happen if you run the service and a test VM? Without any load balancers or caching servers in between? Does it happen if you run the service locally on your workstation? **The more you can isolate the faulty behavior, the easier it is to fix it.** You should remember that problems will happen, it makes sense to spend some time getting ready for them. Setting up a testing environment might take time and effort. So it's a good idea to do this in advance before any problem actually happens. That way you don't need to do it under pressure when your users are complaining that the system's down. Say you're using a database service that's only reachable from inside your cloud network. This means you can't interact with it directly from the outside, only from instances within your cloud infrastructure. If your service starts acting up, you might want to check the responses from the database directly. Rather than going through any of the other back-end servers. To do this, you'll need to have a debugging machine in the network and you'll need to use tools to interact with the database directly. Again, setting the machine up, and learning how to use the tools takes time. So get ahead of the game and do it in advance before any problems come up. You might remember from the troubleshooting and debugging course, that understanding logs is super important for being able to solve problems in IT.

**When you run your service in the cloud, you need to learn where to find the logs. That the provider keeps and what info is available in which logs.** Some cloud providers offer centralized log solutions to collect all your logs in one place. You can have all your notes, send info, warning and error messages to the log collection point. Then, when you're trying to debug a problem, you can easily see everything that was going on when the error occurred. Up next, we'll check out more strategies that you can use to get to the root cause of your cloudy problems.

**Part of the beauty of running services in the Cloud is that you aren't responsible for everything! Most Cloud providers are happy to provide various levels of support.**

**Testing through software is always our best bet in the cloud.**

### Identifying Where the Failure Is Coming From

As we've called out a few times, when we host our services in the cloud, we need to give up part of the control over the infrastructure that we're using. This might be especially noticeable when we're trying to find the root cause of a problem in our service and we don't know if the failure is caused by an error on our side or on the provider side. So what can we do in that case? Problems on the provider side tend to be isolated to geographical regions. 
* If you're seeing weird problems and you have no idea what could be going on, you can try bringing up your service in a different region and checking if the failure happens there too. If it works fine there, it's likely that there's an issue with the cloud infrastructure and you should bring it up with your provider. If it fails in the other regions too, it's likely that it's a problem with your system. Similarly, if you're seeing problems related to resource usage, you can try running the same system in a different machine type and checking how it behaves there.

This could help out, for example, if your service takes too much time to process incoming requests. By changing your service to more powerful machines, you might improve the overall performance. Another option that we've mentioned a bunch is doing rollbacks for the pieces that have recently changed. Having all your infrastructure stored as code in a version control system will let you access the history of the changes to each component in the system. When setting up your service, you should make sure that you can deploy and roll back each individual piece. Imagine you get an alert saying that the web servers in your application are using a lot more memory than they used to you. You don't know why but you know that a new version was deployed a couple of days ago. By rolling back to the previous version, you can verify if that change was at fault or not. If the server's work fine after the rollback, you can investigate the specific changes and try to figure out why they're using so much more memory. If the server's are still using a lot of memory after the rollback, it means something else is up. In an earlier video, we touched briefly upon one popular option when running things on the cloud called containers. **Containers are packaged applications that are shipped together with their libraries and dependencies.** **Each application is executed in a separate container, completely independent of any other applications running on the same machine.** __Now, one of the neat characteristics of containerized applications is that you can deploy the same container to your local workstation to a server running on-premise or to cloud infrastructure provided by different vendors.__ 
* This can be really helpful when trying to understand if the failure is in the code or the infrastructure. You simply deploy the container somewhere else and check if it behaves the same way. When using containers, the typical architecture is to have a lot of small containers that take care of different parts of the service. This means that the overall system can get really complex and when something breaks, it can be hard to identify where the problem is coming from.

* The key to solving problems in the container world is to make sure you have good logs coming in from all of the parts of the system. And, that you can bring up test instances of each of the applications to try things out when necessary. Up next, we'll talk about some of the tools that we can use in the cloud to recover from outages.

**Rollback is the process of restoring a database or program to a previously defined state, usually to recover from an error.**

### Recovering from Failure

When dealing with a complex system, there's a lot of ways it can fail. If we want our service to be reliable, we need to make sure that we can get it up and running as quickly as possible when bad things happen. We'll need to have good backups and a well-documented recovery plan. Backups here doesn't mean just copies of your data. It also means backups for the different pieces of your infrastructure, including the instances that are running the service and the network that's used to connect to the service. Backups of the data your service handles are extremely important. **If you operate a service that stores any kind of data, it's critical that you implement automatic backups and that you periodically check that those backups are working correctly by performing restores.** This helps make sure that you're backing up the right data and that you have well-documented processes for recovering it when things fail. What about the rest of the infrastructure? If you store all your Infrastructure as code, you already have a backup of what your infrastructure should look like. But if your service goes down for some reason, deploying all that infrastructure from scratch might take awhile. 
* That's why many teams keep backup or a secondary instances of their services already up and running. That way, if there's a problem with the primary instance, they can simply point the load balancer or the DNS entries to the secondary instance with minimal disruption to the service. Alternatively, it's common practice to have enough servers running at any time so that if one of them goes down, the others can still handle the traffic, or on a larger scale, have the service running on different data centers around the world, so that if one of the data centers has a problem, the service can still be provided by the instances running in the other data centers. 

If you're running a service on-premise, you might want to have two different connections to the Internet. This way, if the connection offered by one of your ISPs goes down, you can still connect to the Internet through the other one. When you're running on Cloud, you can mostly rely on your Cloud provider having enough network redundancy. But if you really care about your service staying up no matter what, you might want to run your service on two different Cloud vendors so that if one of the providers has a large outage, you can still rely on the other. Now, imagine you're running your service in one data center. Unfortunately, that data center has just suffered a natural disaster, and all of your instances are down. What do you do? First step, deep breath, don't panic. You need to recover your service from scratch, deploying it in a different data center and getting all your data from backups. As long as the backups are available in other data centers and your Infrastructure is fully stored in a version control system, this should be possible. But figuring out how to successfully bring up the whole system from scratch can take awhile. So you don't want to have to scramble to do it when disaster hits. Instead, you should have a documented procedure that explains all of the steps that you need to take. Since systems evolve over time, you need to make sure that this documentation stays up-to-date. One way to do that is to once in a while pretend that you need to recover your service, follow the documented steps, and check if anything is missing or outdated. Systems will fail. A hundred percent availability is simply not an achievable targets, but being prepared for a failure will let you recover your service quickly and keep your users happy. Up next, we've gathered some resources to give you more info on how to debug problems on the Cloud and to be ready for when things fail. After that, the last quiz of the course awaits you.

### Links

Check out the following links for more information:

https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-instances
https://docs.microsoft.com/en-us/azure/virtual-machines/troubleshooting/
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-troubleshoot.htm

### Module 4 Wrap Up: Managing Cloud Instances at Scale

Wow. Look at all that we've covered. In the past videos, we've discussed a lot of different topics related to deploying software in the Cloud. We've learned a bunch of concepts and techniques that we need to take into account when building applications that will run on Cloud, how to keep them running, and how to figure out what's wrong when things don't go according to plan. We started by discussing some of the different options for storing data in the Cloud. We learned how we can use Cloud's flexibility to get the right type of storage and increase it when needed. We then learned about the different approaches to load balancing, which we can use to ensure that our services can be distributed across a number of servers. We also learned how we can get the load balancer to check the health of the back ends to only send request to the servers that are working fine. We looked into how we can build safety into our code changes. We checked out how we can make sure that changes we pushed to production are well-tested, and how we can use development and test environments to let us experiment with changes until we know they'll give a good experience to our users. We then learned about some of the limitations that we can come across when running our services in the Cloud, which are different from the problems we have when running services on physical machines. We also learned some best practices for how to monitor our service, when and how to generate alerts, and we even saw an alert triggering an action. We wrapped up by looking into some of the problems specific to running our instances in the Cloud and how to deal with those. That was a lot of interesting stuff in a short amount of time. Don't you think? Well, we're not done yet. Up next we've got the last graded assessment of the course. In this assessment, you'll put into practice all of the knowledge that you've built up so far. You'll need to figure out what's going on with the deployment that's not behaving as expected and find a way to fix it. But you've got this.